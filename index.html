<!DOCTYPE html>
<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>
      Owain Evans, Machine Learning at University of Oxford (FHI) 
    </title>
    <link href="src/css.css" rel="stylesheet">
    <link rel="stylesheet" href="src/basic-profile.css" media="all">
    <meta name="description" content="">
  
    <meta name="keywords" content="Owain Evans,

Massachusetts Institute of Technology, Philosophy, MIT, rationality, Oxford, Future of Humanity Institute, FHI, Bostrom,
Superintelligence, Value Learning, IRL, Inverse Reinforcement Learning, Joshua Tenenbaum,

Cognitive Science, AI Alignment, AI Safety, Value Alignment, Safe RL, Deliberation, Survey, Grace, Stuhlm&uuml;ller,  Stuhlmueller,

AI, Artificial Intelligence
"/> 
    <!-- <script type="text/javascript" async="" src="src/ga.js"></script><script type="text/javascript"> -->
    <!--   var _gaq = _gaq || []; -->
    <!--   _gaq.push(['_setAccount', 'UA-33562864-2']); -->
    <!--   _gaq.push(['_trackPageview']); -->
    <!--   (function() { -->
    <!--     var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true; -->
    <!--     ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'; -->
    <!--     var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s); -->
    <!--   })(); -->
    <!-- </script> -->
    
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-96028259-1', 'auto');
  ga('send', 'pageview');

</script>
    
  </head>
  <body>
    <div id="wrap">
      <div id="sidebar">

      <div id="photo">
 <img src="faces/owain_small.jpg" style="border: none; width:220px;" alt="Owain Evans">
      </div>
      
        <ul>
          
          <li><a href="#blog">Blog posts</a></li>
          <li><a href="#research">Papers</a></li>
          <li><a href="#talks">Video and slides</a></li>
           <li><a href="#interns">Past Interns</a></li>
          <li><a href="#collaborators">Collaborators</a></li>
          <li><a href="owainevans_cv_2020.pdf">CV (pdf)</a>
        </ul>
      </div>
      <div id="content">
        <h1 id="owain" style="font-weight:normal;">Owain Evans</h1>

<p style="font-size:16px">Senior Research Fellow in Artificial Intelligence, University of Oxford

<p>I'm interested in AI safety, cognitive science, and philosophy. I'm a research fellow at the <a href="https://www.fhi.ox.ac.uk">Future of Humanity Institute</a>, Oxford University and got my PhD at MIT. I previously worked at <a href="https://ought.org">Ought</a>, where I still serve on the Board of Directors.
    
    <p>If you are interested in interning with me, fill out the form <a href="https://www.lesswrong.com/posts/f69LK7CndhSNA7oPn/ai-safety-research-project-ideas">here</a> or <a href="mailto:owaine@gmail.com">email</a> me. My previous interns are listed <a href="#interns">here</a>.

  <!-- I'm a research scientist working on AI Safety. I'm a research associate at the <a href="https://www.fhi.ox.ac.uk">Future of Humanity Institute</a> (directed by <a href="http://www.nickbostrom.com/">Nick Bostrom</a>). My PhD is from MIT, where I worked on <a href="http://web.mit.edu/cocosci/josh.html">cognitive science</a>,
-->
<p>
  <a href="owainevans_cv_2020.pdf">CV</a>
  |
  <a href="mailto:owaine@gmail.com">Email</a>
  | 
  <a href="https://scholar.google.co.uk/citations?user=4VpTwzIAAAAJ&hl=en">Scholar</a>
  |  <a href="https://www.linkedin.com/in/owain-evans-78b210133/">LinkedIn</a>
  |  <a href="https://www.twitter.com/OwainEvans_UK">Twitter</a>


  <h2><a name="highlights">Highlights</a></h2>

  <table border="0" cellspacing="2" cellpadding="2"><tbody>



<tr>
<td valign="top">
    <a href="https://arxiv.org/abs/1705.08807">
            <img src="ai_survey.png" alt="Link" width="130px">
    </a>
</td>
<td valign="top">
    <b> <a href="https://arxiv.org/abs/1705.08807">When Will AI Exceed Human Performance? Evidence from Experts</a></b>
    <p>
We conducted the first large, representative survey of ML researchers on when AI will reach human level on various tasks. The aggregate forecast (median) was 2026 for high-school essays, 2027 for truck-driving, and 2049 for writing a NYT bestseller.
    </p>
</td>
</tr>

<tr>
<td valign="top">
    <a href="visual_aesthetics/sensory-optimization.html">
            <img src="crayon_dog.png" alt="Link" width="130px">
    </a>
</td>
<td valign="top">
    <b> <a href="visual_aesthetics/sensory-optimization.html">Sensory Optimization: Neural Nets as a Model for Understanding and Creating Art</a></b>
    <p>A cognitive science model for how humans understand and create visual art. Artists optimize paintings to be evocative to their own visual system (analagous to Deep Dream and Style Transfer for CNNs).
    </p>
</td>
</tr>


<tr>
<td valign="top">
    <a href="https://arxiv.org/abs/1707.05173">
            <img src="atari.png" alt="Link" width="130px">
    </a>
</td>
<td valign="top">
    <b> <a href="https://arxiv.org/abs/1707.05173">Trial without Error: Towards Safe Reinforcement Learning via Human Intervention</a></b>
    <p>How can an RL agent learn a task without making a single dangerous error? We train a Deep RL agent with a human in the loop and show how to reduce human labor by training a supervised learner to imitate the human.
    </p>
</td>
</tr>




  </tbody></table>


  <h2><a name="blog">Blog posts</a></h2>
  
  
<p>
  <a href="https://www.lesswrong.com/posts/DWgWbXRfXLGHPgZJM/solving-math-problems-by-relay">Solving Math Problems with Relay Teams: An Experiment in Factored Cognition</a>
  <br> (w/ Ben Goldhaber)

<p><a href="https://ought.org/updates/2020-01-11-arguments">Evaluating Arguments One Step at a Time</a>
  <br> (w/ Ought team)
  
<p><a href="https://www.lesswrong.com/posts/hZD6aJfRNZZxSn7d9/quantifying-household-transmission-of-covid-19">Quantifying Household Transmission of Covid</a>

  <p><a href="https://www.lesswrong.com/posts/sg8YjyDBKAF5s2Bgh/neural-nets-as-a-model-for-how-humans-make-and-understand">
      Neural nets as a model for how humans make and understand visual art</a>

  <p><a href="https://www.lesswrong.com/posts/cnC2RMWEGiGpJv8go/model-mis-specification-and-inverse-reinforcement-learning">
      Model Mis-specification and Inverse Reinforcement Learning: Obstacles to Inferring Preferences from Behavior</a>
    <br> (w/ Jacob Steinhardt)



  <h2><a name="research">Papers</a></h2>
  

<p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3627273">Modelling the health and economic impacts of population-wide testing, contact tracing and isolation (PTTI) strategies for Covid-19</a>
     <br> Colbourn T. et al. (2020)
     <br><i>SSRN Preprint</i>
  
   <p><a href="https://www.medrxiv.org/content/10.1101/2020.05.23.20111559v2">Estimating Household Transmission of SARS-CoV-2</a>
     <br> Curmei M., Ilyas A., Evans O., Steinhardt J. (2020)
     <br><i>Medrxiv Preprint</i>

 <p><a href="https://ought.org/updates/2020-01-11-arguments">Evaluating arguments one step at a time</a>
     <br> Saunders, W., Rachbach, B., Evans, O., Miller, Z., Byun, J., Stuhlm&uuml;ller A. (2020)
     <br><i>Ought.org Technical report</i>
     

     <p><a href="visual_aesthetics/sensory-optimization.html">Sensory Optimization: Neural Networks as a Model for Understanding and Creating Art</a>
     <br> Evans, O. (2019)
     <br><i>Arxiv</i>
     <br> (<a href="https://arxiv.org/abs/1911.07068v1">PDF version</a>)


   <p><a href="https://arxiv.org/abs/1907.01475">Generalizing from a few environments in safety-critical reinforcement learning</a>
     <br> Kenton Z., Filos A., Evans O., Gal Y. (2019)
    <br><i>ICLR 2019 (Safe ML Workshop)</i>

     <p><a href="pdfs/evans_ida_projects.pdf">Machine Learning Projects for Iterated Distillation and Amplification </a>
    <br> Evans O., Saunders W., Stuhlm&uuml;ller A. (2019)
    <br><i>FHI Technical Report</i> 

  <p><a href="pdfs/predicting_judgments_final.pdf">Predicting Human Deliberative Judgments with Machine Learning </a>
    <br> Evans O., Stuhlm&uuml;ller A., Cundy C., Carey R., Kenton, Z., McGrath T., Schreiber A. (2018)
    <br><i>FHI Technical Report</i>

  
  <p><a href="https://arxiv.org/pdf/1803.04926v1">Active Reinforcement Learning with Monte-Carlo Tree Search</a>
    <br> Schulze S.,  Evans O. (2018)
    <br><i>Arxiv</i>

    

  <p><a href="https://arxiv.org/abs/1707.05173">
    The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation
    </a>
    <br> Brundage M., Avin S., Clark J., et al. (2018)
    <br><i>Arxiv</i>
    
  <p><a href="https://arxiv.org/abs/1707.05173">Trial without Error: Towards Safe Reinforcement Learning via Human Intervention</a>
    <br> Saunders S., Sastry G., Stuhlm&uuml;ller A., Evans O. (2017)
    <br><i>AAMAS 2018</i>
    <br> (<a href="https://owainevans.github.io/blog/hirl_blog.html">Blogpost</a>, <a href="https://www.youtube.com/playlist?list=PLjs9WCnnR7PCn_Kzs2-1afCsnsBENWqor">Atari Videos</a>,
 <a href="pdfs/owainevans_cambridge.pdf">Slides</a>)

  <p><a href="https://arxiv.org/abs/1705.08807"> When Will AI Exceed Human Performance? Evidence from AI Experts.</a><br>Grace K., Salvatier J., Zhang B., Dafoe A., Evans O. (2017) <br><i>Journal of AI Research (JAIR) 2018. </i>
    <br>(Covered by
    <a href="http://www.bbc.com/capital/story/20170619-how-long-will-it-take-for-your-job-to-be-automated">BBC News</a>,
    <a href="https://www.newscientist.com/article/2133188-ai-will-be-able-to-beat-us-at-everything-by-2060-say-experts/">New Scientist</a>, <a href="http://www.newsweek.com/artificial-intelligence-will-take-our-jobs-2060-618259">Newsweek</a>, and <a href="http://aiimpacts.org/media-discussion-of-2016-espai/">more</a>)

  <p><a href="https://jsteinhardt.wordpress.com/2017/02/07/model-mis-specification-and-inverse-reinforcement-learning/">Model Mis-specification and Inverse Reinforcement Learning</a>. <br>
    <em>(Essay co-authored with Jacob Steinhardt, 2017)</em>. 

<p>
<a href="http://agentmodels.org"> Agentmodels.org: Modeling Agents with Probabilistic Programs.</a>
<br>Evans O., Stuhlm&uuml;ller A., Salvatier J., Filan D. (2017) <br><i>Online Book and Open-source Library</i>
  
<p><a href="https://arxiv.org/abs/1701.04079v1">Agent-Agnostic Human-in-the-Loop Reinforcement Learning.</a>
 <br> Abel D., Salvatier J., Stuhlm&uuml;ller A., Evans O. (2016) <br><i>NIPS Workshop</i>

<p>
  <a href="pdfs/activeRL_nips.pdf">Active Reinforcement Learning: Observing Rewards at a Cost.</a>
  <br>Krueger D., Leike J, Salvatier J., Evans O. (2016) <br><i>NIPS Workshop</i>
  
 <p> <a href="https://arxiv.org/abs/1512.05832">Learning the Preferences of Ignorant, Inconsistent Agents.</a>
  <br>Evans O., Stuhlm&uuml;ller A., Goodman N. (2016) <br><i> AAAI</i>  

<p><a href="https://www.fhi.ox.ac.uk/wp-content/uploads/nips-workshop-2015-website.pdf">Learning the Preferences of Bounded Agents.</a>
  <br>Evans O., Stuhlm&uuml;ller A., Goodman N. (2015) 
  <br><i> NIPS Workshop</i> 
 
 <p><a href="https://www.fhi.ox.ac.uk/wp-content/uploads/pref.pdf">Learning Structured Preferences.</a>
<br>Evans O., Bergen L., Tenenbaum J. (2012)
<br><i>Proceedings of Cognitive Science Society Conference</i> 


<p><a href="http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2009_1192.pdf">Help or hinder: Bayesian models of social goal inference</a>.
<br>Ullman T., Baker C., Macindoe O., Evans O., Goodman N., &amp; Tenenbaum J. (2010)
<br><i>NIPS</i>

<p>
Bayesian Computational Models for Inferring Preferences (2015) <br><i>MIT Dissertation</i> 





<h2><a name="talks">Video and slides</a></h2>

<p><a href="https://towardsdatascience.com/predicting-the-future-of-ai-98deb3c49fe8">Predicting the future of AI</a>
 (YouTube  <a href="Predicting the future of AI">link</a>)
    <br><em>(Towards Data Science Podcast, 2020)</em>

<p><a href="https://www.youtube.com/watch?v=CFLWDaJ5Usc">
    Synergies Between Near-term and Long-term AI Safety (YouTube)</a>
  <br><em>(Future of Life Institute Conference, 2019 in Puerto Rico)</em>

<p><a href="pdfs/psj_slides_owain.pdf">Predicting Slow Judgment</a>
    <br><em>(Slides for talk at "Aligning AI" workshop at NIPS 2017 in Long Beach.)</em>  


    <p><a href="https://www.youtube.com/watch?v=Kukz6bt8IF0&t=1460s">
        Careers in AI safety (YouTube)</a>
      <br><em>(Effective Altruist Global Conference, 2017 in London)</em>
   
<p><a href="pdfs/owainevans_cambridge.pdf">Trial without Error: Towards Safe Reinforcement Learning via Human Intervention</a>
    <br><em>(Slides for talks at Cambridge Centre for the Future of Intelligence and Google Deepmind)</em>  
  
  <p><a href="pdfs/owainevans_ai_corporation_slides.pdf">Automated Corporations and AI Risk</a>
    <br><em>(Informal talk at Oxford University)</em>

  <p><a href="pdfs/owainevans_toronto_slides.pdf">Agent-agnostic Human-in-the-loop Reinforcement Learning</a>
    <br><em>(Slides for talks at U. Toronto and Deepmind)</em>

    
  <p><a href="pdfs/owainevans_aaai_slides.pdf">Learning the Preferences of Ignorant, Inconsistent Agents</a>
    <br><em>(Slides for oral presentation at AAAI 2016)</em>    
  

  <p><a href="pdfs/owainevans_human_preferences.pdf">Learning Human Preferences</a>
    <br><em>(Short talk at MIT)</em>    
  


<h2><a name="interns">Past Interns</a></h2>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-amwm">Name</th>
    <th class="tg-amwm">Year</th>
    <th class="tg-amwm">Current role</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0lax"><a href="https://danielfilan.com/" target="_blank" rel="noopener noreferrer">Daniel Filan</a></td>
    <td class="tg-0lax">2016</td>
    <td class="tg-0lax">PhD student in ML, UC Berkeley (CHAI)<br></td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="http://johnsalvatier.org/" target="_blank" rel="noopener noreferrer">John Salvatier</a></td>
    <td class="tg-0lax">2016</td>
    <td class="tg-0lax">Independent researcher<br></td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://david-abel.github.io/" target="_blank" rel="noopener noreferrer">David Abel</a></td>
    <td class="tg-0lax">2016</td>
    <td class="tg-0lax">Research Scientist, DeepMind (London)</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://scholar.google.com/citations?user=5Uz70IoAAAAJ&hl=en">David Krueger</a>
    <td class="tg-0lax">2016</td>
    <td class="tg-0lax">Lecturer in ML, University of Cambridge</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://www.linkedin.com/in/williamrsaunders/" target="_blank" rel="noopener noreferrer">William Saunders</a></td>
    <td class="tg-0lax">2017</td>
    <td class="tg-0lax">Research Engineer, OpenAI (Alignment Team)</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://www.linkedin.com/in/girish-sastry-2a39348/" target="_blank" rel="noopener noreferrer">Girish Sastry</a></td>
    <td class="tg-0lax">2017</td>
    <td class="tg-0lax">Researcher, OpenAI (Policy Team)</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://www.fhi.ox.ac.uk/team/ryan-carey/" target="_blank" rel="noopener noreferrer">Ryan Carey</a></td>
    <td class="tg-0lax">2017</td>
    <td class="tg-0lax">PhD student in ML (Oxford) and Researcher at FHI</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://cundy.me/" target="_blank" rel="noopener noreferrer">Chris Cundy</a></td>
    <td class="tg-0lax">2017</td>
    <td class="tg-0lax">PhD student in ML, Stanford</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://www.linkedin.com/in/tom-mcgrath-7337bb151/?originalSubdomain=uk" target="_blank" rel="noopener noreferrer">Tom McGrath</a></td>
    <td class="tg-0lax">2018</td>
    <td class="tg-0lax">Research Scientist in AI Safety, DeepMind (London)</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://zackenton.github.io/" target="_blank" rel="noopener noreferrer">Zac Kenton</a></td>
    <td class="tg-0lax">2018</td>
    <td class="tg-0lax">Research Scientist in AI Safety, DeepMind (London)</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://www.linkedin.com/in/richard-ngo-9056b473/?originalSubdomain=uk" target="_blank" rel="noopener noreferrer">Richard Ngo</a></td>
    <td class="tg-0lax">2018</td>
    <td class="tg-0lax">PhD student, University of Cambridge</td>
  </tr>
</tbody>
</table>


<h2><a name="collaborators">Past Collaborators</a></h2>


<ul>
<li><a href="http://cocolab.stanford.edu/ndg.html">Noah Goodman (Stanford)</a>
  <li><a href="https://stuhlmueller.org/">Andreas Stuhlmüller (Ought)</a>
  <li><a href="https://katjagrace.com/">Katja Grace (AI Impacts)</a>
  <li><a href="https://jan.leike.name/">Jan Leike (DeepMind, OpenAI)</a>
  <li><a href="http://www.allandafoe.com/">Allan Dafoe (FHI)</a>
  <li><a href="http://baobaofzhang.github.io/">Baobao Zhang (FHI, MIT)</a>
  <li><a href="https://jsteinhardt.stat.berkeley.edu/">Jacob Steinhardt (Stanford, UC Berkeley)
      <li>Sebastian Schulze (Oxford)
        <li><a href="http://www.cs.ox.ac.uk/people/yarin.gal/website/">Yarin Gal (Oxford)</a>
      <li><a href="https://sites.google.com/view/mihaela-curmei/home">Mihaela Curmei (UC Berkeley)</a>
        <li><a href="http://andrewilyas.com/">Andrew Ilyas (MIT)</a>
</ul>    


        <p class="credits">
          Adapted from Matei Zaharia and <a href="http://andreasviklund.com/">Andreas Viklund</a>.
        </p>
      </div>
    </div>
  

</body></html>
