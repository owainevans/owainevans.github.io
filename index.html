<!DOCTYPE html>
<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>
      Owain Evans, AI Alignment researcher
    </title>
    <link href="src/css.css" rel="stylesheet">
    <link rel="stylesheet" href="src/basic-profile.css" media="all">
    <meta name="description" content="Owain Evans is an AI Alignment researcher leading a new research group in Berkeley and affiliated with Oxford University.
     Discover his publications, blog posts, and collaborative opportunities on AI alignment, AGI risk, and related topics.">

  
    <meta name="keywords" content="Owain Evans,

Reversal Curse, Situational awareness, TruthfulQA, Autocast, Forecasting, MIT, rationality, LessWrong, Oxford, Future of Humanity Institute, FHI, Bostrom,
Superintelligence, Value Learning, IRL, Inverse Reinforcement Learning, truthful, honest, AI, Artificial Intelligence, TruthfulQA,

Cognitive Science, AI Alignment, AI Safety, Value Alignment, Safe RL, Deliberation, Survey, Grace, Stuhlm&uuml;ller,  Stuhlmueller,

AI, Artificial Intelligence
"/> 
    <!-- <script type="text/javascript" async="" src="src/ga.js"></script><script type="text/javascript"> -->
    <!--   var _gaq = _gaq || []; -->
    <!--   _gaq.push(['_setAccount', 'UA-33562864-2']); -->
    <!--   _gaq.push(['_trackPageview']); -->
    <!--   (function() { -->
    <!--     var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true; -->
    <!--     ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'; -->
    <!--     var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s); -->
    <!--   })(); -->
    <!-- </script> -->
    
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-96028259-1', 'auto');
  ga('send', 'pageview');

</script>
    
  </head>
  <body>
    <div id="wrap">
      <div id="sidebar">

      <div id="photo">
 <a href="photos.html" target="_blank"><img src="faces/owain_small.jpg" style="border: none; width:220px;" alt="Owain Evans"></a>
      </div>
      
        <ul>
          
          <li><a href="#blog">Blog posts</a></li>
          <li><a href="#research">Papers</a></li>
          <li><a href="#talks">Video and slides</a></li>
           <li><a href="#interns">Past Mentees</a></li>
          <li><a href="#collaborators">Collaborators</a></li>
          <!-- <li><a href="owainevans_cv_2022.pdf">CV (pdf)</a> -->
        </ul>
      </div>
      <div id="content">
        <h1 id="owain" style="font-weight:normal;">Owain Evans</h1>

<p style="font-size:16px">Director at <a href="https://www.truthfulai.org/" target="_blank">Truthful AI</a> (research group in Berkeley) <br> Affiliate Researcher at CHAI, UC Berkeley

     
  <p style="margin-top: 8px; margin-bottom: 2px;"><b>Recent papers (September 2025):</b></p>
  
  <ul>

    <li>
      <a href="https://arxiv.org/abs/2508.17511">School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior</a>.
             (<a href="https://www.lesswrong.com/posts/CwJ2qWveb9JbaCGQ5/harmless-reward-hacks-can-generalize-to-misalignment-in-llms">blog</a>)
    </li>

    <li>
      <a href="https://arxiv.org/abs/2507.21509">Persona vectors: Monitoring and controlling character traits in LLMs</a>.
             (<a href="https://www.anthropic.com/research/persona-vectors">blog</a>)
    </li>

    <li>
      <a href="https://arxiv.org/abs/2507.14805">Subliminal Learning: LLMs transmit behavioral traits via hidden signals in data</a>.
             (<a href="https://www.lesswrong.com/posts/cGcwQDKAKbQ68BGuR/subliminal-learning-llms-transmit-behavioral-traits-via">blog</a>)
    </li>

    <li>
      <a href="https://arxiv.org/abs/2502.17424">Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs</a>.
             (<a href="https://www.lesswrong.com/posts/ifechgnJRtJdduFGC/emergent-misalignment-narrow-finetuning-can-produce-broadly">blog</a>)
    </li>

  </ul>

     <h4>Work with my team</h4>
   <p> If you want to collaborate or join my team, a good option is the <a href="https://www.constellation.org/programs/astra-fellowship" target="_blank">Astra Fellowship</a>. 
    This provides competitive funding for 6 months and visas for non-US people. It's based in our offices in Berkeley, California. 
    We expect to convert some fellows to full-time positions. Apply by October 10th for the January 2026 cohort.

    <p>If the fellowship is not a good fit (e.g. because you are already an experienced AI Safety researcher), 
      please contact <a href="mailto:owaine@gmail.com" target="_blank">me</a> or my colleague <a href="mailto:chuajamessh@gmail.com" target="_blank">James Chua</a>. 
      We are interested in collaboration and in hiring for research scientist positions

 
 <h4>About Me</h4>        
 

<p>I have a broad interest in AI alignment and AGI risk. 
    My current focus is emergent misalignment, out-of-context reasoning, deception, and situational awareness in AI systems. 
    I run a research non-profit in Berkeley called <a href="https://truthfulai.org/">Truthful AI</a>. 
    I'm also an affiliate of the <a href="https://humancompatible.ai/">CHAI group</a> at UC Berkeley.  
    <p>In the past, I worked on AI Alignment at the University of Oxford (FHI) and earned my PhD at MIT.
       I also worked at <a href="https://ought.org" target="_blank">Ought</a>, 
       where I still serve on the Board of Directors. 
       I post regular research updates on <a href="https://www.twitter.com/OwainEvans_UK" target="_blank">Twitter</a>.
    I've mentored many researchers; previous mentees are listed <a href="#interns">here</a>.
    </p>


<!-- <p>Current collaborators: Alexander Meinke, Rudolf Laine, Jan Brauner, Sören Mindermann, 
<a href="http://www.lorenzopacchiardi.me/">Lorenzo Pacchiardi</a>, 
<a href="https://homepages.inf.ed.ac.uk/s1302760/">Asa Stickland</a>, 
Mikita Balesni, <a href="https://www.linkedin.com/in/lukas-berglund/">Lukas Berglund</a>, 
<a href="https://www.linkedin.com/in/megtong/">Meg Tong</a>, 
<a href="https://maxkaufmann.com/">Max Kaufmann</a>, <a href="https://alexjchan.com/">Alex Chan</a>, 
<a href="https://www.linkedin.com/in/danesherbs/?originalSubdomain=uk">Dane Sherburn</a>. -->

           
  <!-- I'm a research scientist working on AI Safety. I'm a research associate at the <a href="https://www.fhi.ox.ac.uk">Future of Humanity Institute</a> (directed by <a href="http://www.nickbostrom.com/">Nick Bostrom</a>). My PhD is from MIT, where I worked on <a href="http://web.mit.edu/cocosci/josh.html">cognitive science</a>,
-->
<p>

  <a href="mailto:owaine@gmail.com">Email</a>
  | 
  <a href="https://scholar.google.co.uk/citations?user=4VpTwzIAAAAJ&hl=en">Scholar</a>
  |  <a href="https://www.linkedin.com/in/owain-evans-78b210133/">LinkedIn</a>
  |  <a href="https://www.twitter.com/OwainEvans_UK">Twitter</a>
  |  <a href="https://www.lesswrong.com/users/owain_evans">LessWrong</a>


  <h2><a name="highlights">Highlights</a></h2>

  <table border="0" cellspacing="2" cellpadding="2"><tbody>

      <tr>
      <td valign="top">
          <a href="https://arxiv.org/abs/2507.14805">
                  <img src="sublim.png" alt="Link" width="110px" style="border: 1px solid black;">
          </a>
      </td>
      <td valign="top">
          <b> <a href="https://arxiv.org/abs/2507.14805">Subliminal Learning: LLMs transmit behavioral traits via hidden signals in data</a></b>
          <p>
           LLMs can transmit traits to other models via hidden signals in data, even when datasets consist only of simple numerical data.
          </p>
      </td>
      </tr>

      <tr>
      <td valign="top">
          <a href="https://arxiv.org/abs/2502.17424">
                  <img src="EM.png" alt="Link" width="110px" style="border: 1px solid black;">
          </a>
      </td>
      <td valign="top">
          <b> <a href="https://arxiv.org/abs/2502.17424">Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs</a></b>
          <p>
           Models finetuned on narrow misaligned behaviors (like insecure code) can generalize to broader misalignment, including harmful advice and deceptive behavior.
          </p>
      </td>
      </tr>

      <tr>
      <td valign="top">
          <a href="https://arxiv.org/abs/2407.04694">
                  <img src="sad2.png" alt="Link" width="110px" style="border: 1px solid black;">
          </a>
      </td>
      <td valign="top">
          <b> <a href="https://arxiv.org/abs/2407.04694">Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs</a></b>
          <p>
           The first large-scale, multi-task benchmark for situational awareness in LLMs, with 7 task categories and more than 12,000 questions.
          </p>
      </td>
      </tr>

    <tr>

      
<tr>
      <td valign="top">
          <a href="https://arxiv.org/abs/2406.14546">
                  <img src="connecting.png" alt="Link" width="110px" style="border: 1px solid black;">
          </a>
      </td>
      <td valign="top">
          <b> <a href="https://arxiv.org/abs/2406.14546">Connecting the Dots: LLMs can Infer & Verbalize Latent Structure from Training Data</a></b>
          <p>
            LLMs trained only on individual coin flip outcomes can verbalize whether the coin is biased, and those trained only on pairs (x,f(x)) can articulate a definition of f and compute inverses.
          </p>
      </td>
      </tr>

    <tr>


      

    <tr>
      <td valign="top">
          <a href="https://arxiv.org/abs/2309.12288">
                  <img src="reversal.png" alt="Link" width="110px" style="border: 1px solid black;">
          </a>
      </td>
      <td valign="top">
          <b> <a href="https://arxiv.org/abs/2309.12288">The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"</a></b>
          <p>
            If an LLM is trained on "Olaf Scholz was 9th Chancellor of Germany", it will not automatically be able to answer the question, "Who was 9th Chancellor of Germany? 
          </p>
      </td>
      </tr>

    <tr>

    <tr>
      <td valign="top">
          <a href="https://arxiv.org/abs/2309.15840">
                  <img src="clippy_meme.png" alt="Link" width="110px">
          </a>
      </td>
      <td valign="top">
          <b> <a href="https://arxiv.org/abs/2309.15840">How To Catch an AI Liar</a></b>
          <p>
      We create a lie detector for blackbox LLMs by asking models a fixed set of questions (unrelated to the lie).
          </p>
      </td>
      </tr>

<!--       <tr>
        <td valign="top">
            <a href="https://arxiv.org/abs/2309.00667">
                    <img src="sita.png" alt="Link" width="110px" style="border: 1px solid black;">

            </a>
        </td>
        <td valign="top">
            <b> <a href="https://arxiv.org/abs/2309.00667">Taken out of context: On measuring situational awareness in LLMs</a></b>
            <p>
        We define <b>situational awareness</b> and <b>out-of-context reasoning</b>, and investigate how they scale with model size.
            </p>
        </td>
        </tr>

      <tr>
 -->
        

<!-- <td valign="top">
    <a href="https://arxiv.org/abs/2205.14334">
            <img src="calib.png" alt="Link" width="130px">
    </a>
</td>
<td valign="top">
    <b> <a href="https://arxiv.org/abs/2205.14334">Teaching Models to Express Their Uncertainty in Words</a></b>
    <p>
We show that GPT-3 can learn to express uncertainty about its own answers in natural language -- and is moderately calibrated even under distribution shift. 
    </p>
</td>
</tr> -->

<tr>
<td valign="top">
    <a href="https://arxiv.org/abs/2109.07958">
            <img src="gpt3_graph.png" alt="Link" width="130px">
    </a>
</td>
<td valign="top">
    <b> <a href="https://arxiv.org/abs/2109.07958">TruthfulQA: Measuring how models mimic human falsehoods</a></b>
    <p>
New benchmark testing if models like GPT3 are truthful. We find that models fail and imitate human misconceptions. Larger models (with more parameters) do worse. 
    </p>
</td>
</tr>

<tr>
<td valign="top">
    <a href="https://arxiv.org/abs/2110.06674">
            <img src="whitepaper.png" alt="Link" width="130px">
    </a>
</td>
<td valign="top">
    <b> <a href="https://arxiv.org/abs/2110.06674">Truthful AI: Developing and governing AI that does not lie</a></b>
    <p>AI systems are becoming capable of producing personalized deceptive statements at scale. How could we create helpful AI systems that reliably avoid "lying" to humans?
    </p>
</td>
</tr>
      

<!-- <tr>
<td valign="top">
    <a href="https://arxiv.org/abs/1705.08807">
            <img src="ai_survey.png" alt="Link" width="130px">
    </a>
</td>
<td valign="top">
    <b> <a href="https://arxiv.org/abs/1705.08807">When Will AI Exceed Human Performance? Evidence from Experts</a></b>
    <p>
We conducted the first large, representative survey of ML researchers on when AI will reach human level on various tasks. The aggregate forecast (median) was 2026 for high-school essays, 2027 for truck-driving, and 2049 for writing a NYT bestseller.
    </p>
</td>
</tr>

<tr>
<td valign="top">
    <a href="visual_aesthetics/sensory-optimization.html">
            <img src="crayon_dog.png" alt="Link" width="130px">
    </a>
</td>
<td valign="top">
    <b> <a href="visual_aesthetics/sensory-optimization.html">Sensory Optimization: Neural Nets as a Model for Understanding and Creating Art</a></b>
    <p>A cognitive science model for how humans understand and create visual art. Artists optimize paintings to be evocative to their own visual system (analagous to Deep Dream and Style Transfer for CNNs).
    </p>
</td>
</tr>


<tr>
<td valign="top">
    <a href="https://arxiv.org/abs/1707.05173">
            <img src="atari.png" alt="Link" width="130px">
    </a>
</td>
<td valign="top">
    <b> <a href="https://arxiv.org/abs/1707.05173">Trial without Error: Towards Safe Reinforcement Learning via Human Intervention</a></b>
    <p>How can an RL agent learn a task without making a single dangerous error? We train a Deep RL agent with a human in the loop and show how to reduce human labor by training a supervised learner to imitate the human.
    </p>
</td>
</tr> -->




  </tbody></table>


  <h2><a name="blog">Blog posts</a></h2>

  <h3>Blogposts about our papers</h3>

  <p>
    <a href="https://www.lesswrong.com/posts/CwJ2qWveb9JbaCGQ5/harmless-reward-hacks-can-generalize-to-misalignment-in-llms">Harmless reward hacks can generalize to misalignment in LLMs</a>
  <p>
    <a href="https://www.anthropic.com/research/persona-vectors">Persona vectors: Monitoring and controlling character traits in LLMs</a>
  <p>
    <a href="https://www.lesswrong.com/posts/cGcwQDKAKbQ68BGuR/subliminal-learning-llms-transmit-behavioral-traits-via">Subliminal Learning: LLMs Transmit Behavioral Traits via Hidden Signals in Data</a>
  <p>
    <a href="https://www.lesswrong.com/posts/HHhGaJszSG7cburJ6/backdoor-awareness-and-misaligned-personas-in-reasoning">Backdoor awareness and misaligned personas in reasoning models</a>
  <p>
    <a href="https://www.lesswrong.com/posts/zzZ6jye3ukiNyMCmC/thought-crime-backdoors-and-emergent-misalignment-in">Thought Crime: Backdoors & Emergent Misalignment in Reasoning Models</a>
  <p>
    <a href="https://www.lesswrong.com/posts/ifechgnJRtJdduFGC/emergent-misalignment-narrow-finetuning-can-produce-broadly">Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs</a>
  <p>
    <a href="https://www.lesswrong.com/posts/xrv2fNJtqabN3h6Aj/tell-me-about-yourself-llms-are-aware-of-their-learned">Tell me about yourself: LLMs are aware of their learned behaviors</a>
  <p>
    <a href="https://www.lesswrong.com/posts/C8HAa2mf5kcBrpjkX/inference-time-compute-more-faithful-a-research-note">Inference-Time-Compute: More Faithful? A Research Note</a>
  <p>
    <a href="https://www.lesswrong.com/posts/L3aYFT4RDJYHbbsup/llms-can-learn-about-themselves-by-introspection">LLMs can learn about themselves by introspection</a>
  <p>
    <a href="https://www.lesswrong.com/posts/YsCRXZYr5DcJ84XHq/me-myself-and-ai-the-situational-awareness-dataset-sad-for">Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs</a>
  <p>
    <a href="https://www.lesswrong.com/posts/khFC2a4pLPvGtXAGG/how-to-catch-an-ai-liar-lie-detection-in-black-box-llms-by">How to catch an AI liar: Lie detection in black-box LLMs by asking unrelated questions</a>
  <p>
    <a href="https://www.lesswrong.com/posts/SCqDipWAhZ49JNdmL/paper-llms-trained-on-a-is-b-fail-to-learn-b-is-a">LLMs trained on "A is B" fail to learn "B is A" (The Reversal Curse)</a>
  <p>
    <a href="https://www.lesswrong.com/posts/PF58wEdztZFX2dSue/how-truthful-is-gpt-3-a-benchmark-for-language-models">How truthful is GPT-3? A benchmark for language models</a>
  <p>
    <a href="https://www.lesswrong.com/posts/aBixCPqSnTsPsTJBQ/truthful-ai-developing-and-governing-ai-that-does-not-lie">Truthful AI: Developing and governing AI that does not lie</a>

  <h3>Other blogposts</h3>

  <p>
    <a href="https://www.lesswrong.com/posts/G4qwcfdRAyDFcqRiM/concept-poisoning-probing-llms-without-probes">(Research update) Concept Poisoning: Probing LLMs without probes</a>
  <p>
    <a href="https://www.lesswrong.com/posts/Bunfwz6JsNd44kgLT/new-improved-multiple-choice-truthfulqa">(Research update) New, improved multiple-choice TruthfulQA</a>
  <p>
    <a href="https://www.lesswrong.com/posts/i3b9uQfjJjJkwZF4f/tips-on-empirical-research-slides">Tips On Empirical Research Slides</a>
  <p>
    <a href="talk-transcript.html">Vintage LLMs: Pretrain language models on data up to a particular date</a>
  <p>
    <a href="https://www.lesswrong.com/posts/ZKksgfTxuxKhDfk4m/how-do-llms-give-truthful-answers-a-discussion-of-llm-vs">How do LLMs give truthful answers? A discussion of LLM vs human reasoning, ensembles & parrots</a>
  <p>
    <a href="https://www.lesswrong.com/posts/yYkrbS5iAwdEQyynW/how-do-new-models-from-openai-deepmind-and-anthropic-perform">(Research update) How do new models from OpenAI, DeepMind and Anthropic perform on TruthfulQA?</a>
  <p>
    <a href="https://owainevans.github.io/ModernistPoetrybyGPT3.html">Modernist poetry by GPT-3 davinci</a>
  <p>
    <a href="https://www.lesswrong.com/posts/qbHLGo5vu8HD3JqEM/lives-of-the-cambridge-polymath-geniuses">Lives of the Cambridge Polymath Geniuses</a>
  <p>
    <a href="https://www.lesswrong.com/posts/DWgWbXRfXLGHPgZJM/solving-math-problems-by-relay">Solving Math Problems with Relay Teams: An Experiment in Factored Cognition</a>
    <br> (w/ Ben Goldhaber)
  <p>
    <a href="https://ought.org/updates/2020-01-11-arguments">Evaluating Arguments One Step at a Time</a>
    <br> (w/ Ought team)
  <p>
    <a href="https://www.lesswrong.com/posts/hZD6aJfRNZZxSn7d9/quantifying-household-transmission-of-covid-19">Quantifying Household Transmission of Covid</a>
  <p>
    <a href="https://www.lesswrong.com/posts/sg8YjyDBKAF5s2Bgh/neural-nets-as-a-model-for-how-humans-make-and-understand">
      Neural nets as a model for how humans make and understand visual art</a>
  <p>
    <a href="https://www.lesswrong.com/posts/cnC2RMWEGiGpJv8go/model-mis-specification-and-inverse-reinforcement-learning">
      Model Mis-specification and Inverse Reinforcement Learning: Obstacles to Inferring Preferences from Behavior</a>
    <br> (w/ Jacob Steinhardt)

     <p>More posts <a href="https://www.lesswrong.com/users/owain_evans">here</a>.



       
       <h2><a name="research">Papers</a></h2>

       <p><a href="https://arxiv.org/abs/2411.16353">Lessons from Studying Two-Hop Latent Reasoning</a>
        <br>
        M Balesni, T Korbak, O Evans (2025)
     <br><i>arXiv preprint arXiv:2411.16353</i>
     <br> (<a href="pdfs/lessons_two_hop.pdf">PDF</a>)
 
       <p><a href="https://arxiv.org/abs/2508.17511">School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs</a>
        <br>
        M Taylor, J Chua, J Betley, J Treutlein, O Evans (2025)
     <br><i>arXiv preprint arXiv:2508.17511</i>

       <p><a href="https://arxiv.org/abs/2507.21509">Persona vectors: Monitoring and controlling character traits in language models</a>
        <br>
        R Chen, A Arditi, H Sleight, O Evans, J Lindsey (2025)
     <br><i>arXiv preprint arXiv:2507.21509</i>

       <p><a href="https://arxiv.org/abs/2507.14805">Subliminal Learning: Language models transmit behavioral traits via hidden signals in data</a>
        <br>
        A Cloud, M Le, J Chua, J Betley, A Sztyber-Betley, J Hilton, S Marks, O Evans (2025)
     <br><i>arXiv preprint arXiv:2507.14805</i>

       <p><a href="https://arxiv.org/abs/2507.11473">Chain of thought monitorability: A new and fragile opportunity for ai safety</a>
        <br>
        T Korbak, M Balesni, E Barnes, Y Bengio, J Benton, J Bloom, M Chen, O Evans (2025)
     <br><i>arXiv preprint arXiv:2507.11473</i>

       <p><a href="https://arxiv.org/abs/2506.13206">Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models</a>
        <br>
        J Chua, J Betley, M Taylor, O Evans (2025)
     <br><i>arXiv preprint arXiv:2506.13206</i>

       <p><a href="https://arxiv.org/abs/2502.17424">Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs</a>
        <br>
        J Betley, D Tan, N Warncke, A Sztyber-Betley, X Bao, M Soto, N Labenz, O Evans (2025)
     <br><i>ICML 2025 (Oral)</i>

       <p><a href="https://arxiv.org/abs/2501.11120">Tell me about yourself: LLMs are aware of their learned behaviors</a>
        <br>
        J Betley, X Bao, M Soto, A Sztyber-Betley, J Chua, O Evans (2025)
     <br><i>ICLR 2025</i>

       <p><a href="https://arxiv.org/abs/2501.08156">Are DeepSeek R1 And Other Reasoning Models More Faithful?</a>
        <br>
        J Chua, O Evans (2025)
     <br><i>arXiv preprint arXiv:2501.08156</i>

       <p><a href="https://arxiv.org/abs/2410.13787">Looking Inward: Language Models Can Learn About Themselves by Introspection</a>
        <br>
        Binder, F., Chua, J., Korbak, T.; Sleight, H., Hughes, J., Long, R., Perez, E., Turpin, M., Evans, O. (2024)
     <br><i>ICLR 2025</i>
         <p><a href="https://arxiv.org/abs/2407.04694">Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs</a>
   <br>
    Laine, R., Chughtai, B., Betley, J., Hariharan, K., Scheurer, J., Balesni, M., Hobbhahn, M., Meinke, A., Evans, O. (2024)
<br><i>NeurIPS 2024</i>
<p><a href="https://arxiv.org/abs/2406.14546">Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data</a>
   <br>
    Treutlein, J., Choi, D., Betley, J., Anil, C., Marks, S., Grosse, RB., Evans, O. (2024)
<br><i>NeurIPS 2024</i>
<p><a href="https://arxiv.org/abs/2405.07436">Can Language Models Explain Their Own Classification Behavior?</a>
   <br>
    Sherburn, D., Chughtai, B., Evans, O. (2024)
<br><i>arXiv preprint arXiv:2405.07436</i>
<p><a href="https://arxiv.org/abs/2312.07779">Tell, Don't show: Declarative facts influence how LLMs generalize</a>
   <br>
    Meinke, A., Evans, O. (2023)
<br><i>arXiv preprint arXiv:2312.07779</i>
<p><a href="https://arxiv.org/abs/2309.15840">How to catch an ai liar: Lie detection in black-box llms by asking unrelated questions</a>
   <br>
    Pacchiardi, L., Chan, AJ., Mindermann, S., Moscovitz, I., Pan, AY., Gal, Y., Evans, O., Brauner, J. (2023)
<br><i>ICLR 2024</i>
<p><a href="https://arxiv.org/abs/2309.12288">The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"</a>
   <br>
    Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stickland, AC., Korbak, T., Evans, O. (2023)
<br><i>ICLR 2024</i>
<p><a href="https://arxiv.org/abs/2309.00667">Taken out of context: On measuring situational awareness in LLMs</a>
   <br>
    Berglund, L., Stickland, AC., Balesni, M., Kaufmann, M., Tong, M., Korbak, T., Kokotajlo, D., Evans, O. (2023)
<br><i>arXiv preprint arXiv:2309.00667</i>
       

        <p><a href="https://arxiv.org/abs/2206.15474">Forecasting Future World Events with Neural Networks</a>
          <br>
           Zou A, Xiao T, Jia R, Kwon J, Mazeika M, Li R, Song D, Steinhardt J, Evans O, Hendrycks D (2022)
    <br><i>Neurips 2022</i>
       

       <p><a href="https://arxiv.org/abs/2205.14334">Teaching Models to Express Their Uncertainty in Words</a>
          <br>
            Lin S., Hilton J., Evans O. (2022)
    <br><i>Transactions of Machine Learning Research</i>

        <p><a href="https://arxiv.org/abs/2110.06674">Truthful AI: Developing and governing AI that does not lie</a>
          <br>
           Evans O., Cotton-Barratt O., Finnveden L., Bales A., Balwit A., Wills P., Righetti L., Saunders W. (2021)
    <br><i>ArXiv</i>
  
  <p><a href="https://arxiv.org/abs/2109.07958">TruthfulQA: Measuring how models mimic human falsehoods</a>
    <br>  Lin S., Hilton J., Evans O. (2021)
    <br><i>ACL</i>
  
<p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3627273">Modelling the health and economic impacts of population-wide testing, contact tracing and isolation (PTTI) strategies for Covid-19</a>
     <br> Colbourn T. et al. (2020)
     <br><i>SSRN Preprint</i>
  
   <p><a href="https://www.medrxiv.org/content/10.1101/2020.05.23.20111559v2">Estimating Household Transmission of SARS-CoV-2</a>
     <br> Curmei M., Ilyas A., Evans O., Steinhardt J. (2020)
     <br><i>International Journal of Epidemiology</i>

 <p><a href="https://ought.org/updates/2020-01-11-arguments">Evaluating arguments one step at a time</a>
     <br> Saunders, W., Rachbach, B., Evans, O., Miller, Z., Byun, J., Stuhlm&uuml;ller A. (2020)
     <br><i>Ought.org Technical report</i>
     

     <p><a href="visual_aesthetics/sensory-optimization.html">Sensory Optimization: Neural Networks as a Model for Understanding and Creating Art</a>
     <br> Evans, O. (2019)
     <br><i>Arxiv</i>
     <br> (<a href="https://arxiv.org/abs/1911.07068v1">PDF version</a>)


   <p><a href="https://arxiv.org/abs/1907.01475">Generalizing from a few environments in safety-critical reinforcement learning</a>
     <br> Kenton Z., Filos A., Evans O., Gal Y. (2019)
    <br><i>ICLR 2019 (Safe ML Workshop)</i>

     <p><a href="pdfs/evans_ida_projects.pdf">Machine Learning Projects for Iterated Distillation and Amplification </a>
    <br> Evans O., Saunders W., Stuhlm&uuml;ller A. (2019)
    <br><i>FHI Technical Report</i> 

  <p><a href="pdfs/predicting_judgments_final.pdf">Predicting Human Deliberative Judgments with Machine Learning </a>
    <br> Evans O., Stuhlm&uuml;ller A., Cundy C., Carey R., Kenton, Z., McGrath T., Schreiber A. (2018)
    <br><i>FHI Technical Report</i>

  
  <p><a href="https://arxiv.org/pdf/1803.04926v1">Active Reinforcement Learning with Monte-Carlo Tree Search</a>
    <br> Schulze S.,  Evans O. (2018)
    <br><i>ArXiv</i>

    

  <p><a href="https://arxiv.org/abs/1707.05173">
    The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation
    </a>
    <br> Brundage M., Avin S., Clark J., et al. (2018)
    <br><i>ArXiv</i>
    
  <p><a href="https://arxiv.org/abs/1707.05173">Trial without Error: Towards Safe Reinforcement Learning via Human Intervention</a>
    <br> Saunders S., Sastry G., Stuhlm&uuml;ller A., Evans O. (2017)
    <br><i>AAMAS 2018</i>
    <br> (<a href="https://owainevans.github.io/blog/hirl_blog.html">Blogpost</a>, <a href="https://www.youtube.com/playlist?list=PLjs9WCnnR7PCn_Kzs2-1afCsnsBENWqor">Atari Videos</a>,
 <a href="pdfs/owainevans_cambridge.pdf">Slides</a>)

  <p><a href="https://arxiv.org/abs/1705.08807"> When Will AI Exceed Human Performance? Evidence from AI Experts.</a><br>Grace K., Salvatier J., Zhang B., Dafoe A., Evans O. (2017) <br><i>Journal of AI Research (JAIR) 2018. </i>
    <br>(Covered by
    <a href="http://www.bbc.com/capital/story/20170619-how-long-will-it-take-for-your-job-to-be-automated">BBC News</a>,
    <a href="https://www.newscientist.com/article/2133188-ai-will-be-able-to-beat-us-at-everything-by-2060-say-experts/">New Scientist</a>, <a href="http://www.newsweek.com/artificial-intelligence-will-take-our-jobs-2060-618259">Newsweek</a>, and <a href="http://aiimpacts.org/media-discussion-of-2016-espai/">more</a>)

  <p><a href="https://jsteinhardt.wordpress.com/2017/02/07/model-mis-specification-and-inverse-reinforcement-learning/">Model Mis-specification and Inverse Reinforcement Learning</a>. <br>
    <em>(Essay co-authored with Jacob Steinhardt, 2017)</em>. 

<p>
<a href="http://agentmodels.org"> Agentmodels.org: Modeling Agents with Probabilistic Programs.</a>
<br>Evans O., Stuhlm&uuml;ller A., Salvatier J., Filan D. (2017) <br><i>Online Book and Open-source Library</i>
  
<p><a href="https://arxiv.org/abs/1701.04079v1">Agent-Agnostic Human-in-the-Loop Reinforcement Learning.</a>
 <br> Abel D., Salvatier J., Stuhlm&uuml;ller A., Evans O. (2016) <br><i>NeurIPS Workshop</i>

<p>
  <a href="https://arxiv.org/abs/2011.06709">Active Reinforcement Learning: Observing Rewards at a Cost.</a>
  <br>Krueger D., Leike J, Salvatier J., Evans O. (2016) <br><i>NeurIPS Workshop</i>
  
 <p> <a href="https://arxiv.org/abs/1512.05832">Learning the Preferences of Ignorant, Inconsistent Agents.</a>
  <br>Evans O., Stuhlm&uuml;ller A., Goodman N. (2016) <br><i>AAAI Conference on Artificial Intelligence,</i>  

<p><a href="pdfs/preferences_bounded_agents_evans.pdf">Learning the Preferences of Bounded Agents.</a>
  <br>Evans O., Stuhlm&uuml;ller A., Goodman N. (2015) 
  <br><i> NeurIPS Workshop</i> 
 
 <p><a href="pdfs/learning_structured_preferences_evans.pdf">Learning Structured Preferences.</a>
<br>Evans O., Bergen L., Tenenbaum J. (2012)
<br><i>Proceedings of Cognitive Science Society Conference</i> 


<p><a href="pdfs/help_hinder_evans.pdf">Help or hinder: Bayesian models of social goal inference</a>.
<br>Ullman T., Baker C., Macindoe O., Evans O., Goodman N., &amp; Tenenbaum J. (2010)
<br><i>NeurIPS</i>

<p>
Bayesian Computational Models for Inferring Preferences (2015) <br><i>MIT Dissertation</i> 





<h2><a name="talks">Video and slides</a></h2>

    <p><a href="https://www.youtube.com/watch?v=pimIny8jJd8&pp=ygUOb3dhaW4gZXZhbnMgYWk%3D">Owain Evans – Emergent Misalignment [Alignment Workshop]</a>
     <br><em>Talk on how fine-tuning on insecure code can induce emergent misalignment across models/domains. (May 2025)</em>

    <p><a href="https://www.youtube.com/watch?v=0ONSOMf5jh4&pp=ygUOb3dhaW4gZXZhbnMgYWk%3D">Owain Evans – Deluding AIs [ControlConf]</a>
     <br><em>How planting false beliefs in AI systems might block weaponization, aid monitoring, and handle out-of-context reasoning. (May 2025)</em>

    <p><a href="https://www.youtube.com/watch?v=3D4pgIKR4cQ&pp=ygUOb3dhaW4gZXZhbnMgYWk%3D">AXRP 42 – Owain Evans on LLM Psychology</a>
     <br><em>Why introspection, experiments from "Looking Inward," whether to fine-tune for introspection, and implications of emergent misalignment. (June 2025)</em>

    <p><a href="https://www.youtube.com/watch?v=eb2oLHblrHU">Video: Podcast Interview on Situational Awareness and Out-of-context Reasoning</a>
     <br><em>(August 2024)</em>

<p><a href="https://www.youtube.com/watch?v=-8mKJd2SwI4">Video talk: Out-of-context Reasoning in LLMs</a>
     <br><em>(New Orleans Alignment Workshop, December 2023)</em>
    
<p><a href="https://www.youtube.com/watch?v=AkLkZgsaKp4">Video talk: Truthful Language Models and Alignment</a>
     <br><em>(University of Toronto, 2023)</em>
 
     <p><a href="https://youtu.be/o6CjRA2KpX0">Video conversation: LLMs, truthful AI, and composition</a>
      <br><em>(Conversation with Ozzie Gooen, 2023)</em>
 

<p><a href="https://towardsdatascience.com/predicting-the-future-of-ai-98deb3c49fe8">Predicting the future of AI</a>
 (YouTube  <a href="https://www.youtube.com/watch?v=dea1XQojRmw&feature=emb_title">link</a>)
    <br><em>(Towards Data Science Podcast, 2020)</em>

<p><a href="https://www.youtube.com/watch?v=CFLWDaJ5Usc">
    Synergies Between Near-term and Long-term AI Safety (YouTube)</a>
  <br><em>(Future of Life Institute Conference, 2019 in Puerto Rico)</em>

<p><a href="pdfs/psj_slides_owain.pdf">Predicting Slow Judgment</a>
    <br><em>(Slides for talk at "Aligning AI" workshop at NeurIPS 2017 in Long Beach.)</em>  


    <p><a href="https://www.youtube.com/watch?v=Kukz6bt8IF0&t=1460s">
        Careers in AI safety (YouTube)</a>
      <br><em>(Effective Altruist Global Conference, 2017 in London)</em>
   
<p><a href="pdfs/owainevans_cambridge.pdf">Trial without Error: Towards Safe Reinforcement Learning via Human Intervention</a>
    <br><em>(Slides for talks at Cambridge Centre for the Future of Intelligence and Google Deepmind)</em>  
  
  <p><a href="pdfs/owainevans_ai_corporation_slides.pdf">Automated Corporations and AI Risk</a>
    <br><em>(Informal talk at Oxford University)</em>

  <p><a href="pdfs/owainevans_toronto_slides.pdf">Agent-agnostic Human-in-the-loop Reinforcement Learning</a>
    <br><em>(Slides for talks at U. Toronto and Deepmind)</em>

    
  <p><a href="pdfs/owainevans_aaai_slides.pdf">Learning the Preferences of Ignorant, Inconsistent Agents</a>
    <br><em>(Slides for oral presentation at AAAI 2016)</em>    
  

  <p><a href="pdfs/owainevans_human_preferences.pdf">Learning Human Preferences</a>
    <br><em>(Short talk at MIT)</em>    
  


<h2><a name="interns">Mentees</a></h2>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-amwm">Name</th>
    <th class="tg-amwm">Year</th>
    <th class="tg-amwm">Current role</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0lax"><a href="https://adamkarvonen.github.io/" target="_blank" rel="noopener noreferrer">Adam Karvonen</a></td>
    <td class="tg-0lax">2025</td>
    <td class="tg-0lax">MATS scholar</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://github.com/Dylan102938" target="_blank" rel="noopener noreferrer">Dylan Feng</a></td>
    <td class="tg-0lax">2025</td>
    <td class="tg-0lax">MATS scholar</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://seas.harvard.edu/person/jorio-cocola" target="_blank" rel="noopener noreferrer">Jorio Coccola</a></td>
    <td class="tg-0lax">2025</td>
    <td class="tg-0lax">MATS scholar</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://x.com/minhxle1" target="_blank" rel="noopener noreferrer">Minh Le</a></td>
    <td class="tg-0lax">2025</td>
    <td class="tg-0lax">Anthropic</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://arxiv.org/abs/2410.04332" target="_blank" rel="noopener noreferrer">Alex Cloud</a></td>
    <td class="tg-0lax">2025</td>
    <td class="tg-0lax">Anthropic</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://dtch1997.github.io/" target="_blank" rel="noopener noreferrer">Daniel Tan</a></td>
    <td class="tg-0lax">2025</td>
    <td class="tg-0lax">PhD student, UCL</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://martin-soto.com" target="_blank" rel="noopener noreferrer">Martín Soto</a></td>
    <td class="tg-0lax">2024-2025</td>
    <td class="tg-0lax">Research Scientist, UK AISI</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://www.cs.toronto.edu/~jennybao/" target="_blank" rel="noopener noreferrer">Jenny (Xuchan) Bao</a></td>
    <td class="tg-0lax">2024-2025</td>
    <td class="tg-0lax">PhD student, Univ. of Toronto</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://www.linkedin.com/in/choidami/" target="_blank" rel="noopener noreferrer">Dami Choi</a></td>
    <td class="tg-0lax">2024</td>
    <td class="tg-0lax">Transluce</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://jameschua.net/about/" target="_blank" rel="noopener noreferrer">James Chua</a></td>
    <td class="tg-0lax">2024</td>
    <td class="tg-0lax">Truthful AI</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://johannestreutlein.com/" target="_blank" rel="noopener noreferrer">Johannes Treutlein</a></td>
    <td class="tg-0lax">2024</td>
    <td class="tg-0lax">Anthropic</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://www.janbetley.net/" target="_blank" rel="noopener noreferrer">Jan Betley</a></td>
    <td class="tg-0lax">2024</td>
    <td class="tg-0lax">Truthful AI</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://ac.felixbinder.net/" target="_blank" rel="noopener noreferrer">Felix Binder</a></td>
    <td class="tg-0lax">2024</td>
    <td class="tg-0lax">Meta AI</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://alexmeinke.de" target="_blank" rel="noopener noreferrer">Alexander Meinke</a></td>
    <td class="tg-0lax">2023</td>
    <td class="tg-0lax">Research Scientist, Apollo Research</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://www.lorenzopacchiardi.me" target="_blank" rel="noopener noreferrer">Lorenzo Pacchiardi</a></td>
    <td class="tg-0lax">2023</td>
    <td class="tg-0lax">Research Associate, Univ. of Cambridge</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://www.aisi.gov.uk/people/asa-cooper-stickland" target="_blank" rel="noopener noreferrer">Asa Cooper Stickland</a></td>
    <td class="tg-0lax">2023</td>
    <td class="tg-0lax">Research Scientist, UK AI Safety Institute (AISI)</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://mbalesni.github.io" target="_blank" rel="noopener noreferrer">Mikita Balesni</a></td>
    <td class="tg-0lax">2023</td>
    <td class="tg-0lax">Research Scientist & founding member, Apollo Research</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://scholar.google.com/citations?user=OOIiDc0AAAAJ" target="_blank" rel="noopener noreferrer">Lukas Berglund</a></td>
    <td class="tg-0lax">2023</td>
    <td class="tg-0lax">U.S. AI Safety Institute (NIST AISI)</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://www.megtong.com" target="_blank" rel="noopener noreferrer">Meg Tong</a></td>
    <td class="tg-0lax">2023</td>
    <td class="tg-0lax">Anthropic</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://maxkaufmann.com" target="_blank" rel="noopener noreferrer">Max Kaufmann</a></td>
    <td class="tg-0lax">2023</td>
    <td class="tg-0lax">PhD student, Univ. of Toronto, ex: UK AISI</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://alexjchan.com" target="_blank" rel="noopener noreferrer">Alex J. Chan</a></td>
    <td class="tg-0lax">2023</td>
    <td class="tg-0lax">Salesforce, ex-Spotify</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://tomekkorbak.com" target="_blank" rel="noopener noreferrer">Tomek Korbak</a></td>
    <td class="tg-0lax">2023</td>
    <td class="tg-0lax">Senior Research Scientist, UK AISI (ex-Anthropic)</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://openreview.net/profile?id=~Alexa_Yue_Pan1" target="_blank" rel="noopener noreferrer">Alexa (Yue) Pan</a></td>
    <td class="tg-0lax">2023</td>
    <td class="tg-0lax">Redwood Research</td>
  </
  tr>
  <tr>
    <td class="tg-0lax"><a href="https://danesherbs.com" target="_blank" rel="noopener noreferrer">Dane Sherburn</a></td>
    <td class="tg-0lax">2022-2023</td>
    <td class="tg-0lax">OpenAI</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://scholar.google.com/citations?user=lSNRNNYAAAAJ" target="_blank" rel="noopener noreferrer">Stephanie Lin</a></td>
    <td class="tg-0lax">2021-2022</td>
    <td class="tg-0lax">OpenAI</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://lukasfinnveden.substack.com" target="_blank" rel="noopener noreferrer">Lukas Finnveden</a></td>
    <td class="tg-0lax">2021-2022</td>
    <td class="tg-0lax">Research Analyst, Redwood Research</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://universalprior.substack.com" target="_blank" rel="noopener noreferrer">Jan Hendrik Kirchner</a></td>
    <td class="tg-0lax">2022</td>
    <td class="tg-0lax">Researcher at Anthropic (ex-OpenAI)</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://tommcgrath.github.io" target="_blank" rel="noopener noreferrer">Tom McGrath</a></td>
    <td class="tg-0lax">2018</td>
    <td class="tg-0lax">Chief Scientist & Co-founder, Goodfire, ex-GDM</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="" target="_blank" rel="noopener noreferrer">Zac Kenton</a></td>
    <td class="tg-0lax">2018</td>
    <td class="tg-0lax">Staff Research Scientist, Google DeepMind</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://www.richardcngo.com" target="_blank" rel="noopener noreferrer">Richard Ngo</a></td>
    <td class="tg-0lax">2018</td>
    <td class="tg-0lax">Independent; previously OpenAI Governance</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://openreview.net/profile?id=~William_Saunders1" target="_blank" rel="noopener noreferrer">William Saunders</a></td>
    <td class="tg-0lax">2017</td>
    <td class="tg-0lax">Researcher, Alignment Science, Anthropic, ex-OpenAI</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://x.com/girishsastry" target="_blank" rel="noopener noreferrer">Girish Sastry</a></td>
    <td class="tg-0lax">2017</td>
    <td class="tg-0lax">Independent researcher/policy, ex-OpenAI</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://x.com/nealjean1" target="_blank" rel="noopener noreferrer">Neal Jean</a></td>
    <td class="tg-0lax">2017</td>
    <td class="tg-0lax">Co-founder & CEO, Beacons</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://ryancarey.github.io" target="_blank" rel="noopener noreferrer">Ryan Carey</a></td>
    <td class="tg-0lax">2017</td>
    <td class="tg-0lax">Optiver, ex-Oxford PhD</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://cundy.me" target="_blank" rel="noopener noreferrer">Chris Cundy</a></td>
    <td class="tg-0lax">2017</td>
    <td class="tg-0lax">Research Scientist, FAR AI</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://danielfilan.com" target="_blank" rel="noopener noreferrer">Daniel Filan</a></td>
    <td class="tg-0lax">2016</td>
    <td class="tg-0lax">Senior Research Manager, MATS</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://johnsalvatier.org" target="_blank" rel="noopener noreferrer">John Salvatier</a></td>
    <td class="tg-0lax">2016</td>
    <td class="tg-0lax">Independent researcher</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://david-abel.github.io" target="_blank" rel="noopener noreferrer">David Abel</a></td>
    <td class="tg-0lax">2016</td>
    <td class="tg-0lax">Senior Research Scientist at Google DeepMind</td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://davidscottkrueger.com" target="_blank" rel="noopener noreferrer">David Krueger</a></td>
    <td class="tg-0lax">2016</td>
    <td class="tg-0lax">Assistant Professor, Mila, ex-Cambridge</td>
  </tr>
</tbody>



</table>



<!-- <p><br>
  In 2021-2022 I worked with
<a href="https://www.linkedin.com/in/sylin07/" target="_blank" rel="noopener noreferrer">
  Stephanie Lin</a> (now OpenAI Superalignment Team) and

<a href="https://www.linkedin.com/in/lukas-finnveden-787a07145/" target="_blank" rel="noopener noreferrer">
Lukas Finnveden</a> (now Open Philanthropy) who were research scholars at FHI.  -->


<h2><a name="collaborators">Past Collaborators</a></h2>


<ul>
<li><a href="https://cocolab.stanford.edu/ndg">Noah Goodman (Stanford)</a>
  <li><a href="https://stuhlmueller.org/">Andreas Stuhlmüller (Elicit)</a>
  <li><a href="https://katjagrace.com/">Katja Grace (AI Impacts)</a>
  <li><a href="https://jan.leike.name/">Jan Leike (Anthropic)</a>
  <li><a href="https://www.allandafoe.com/">Allan Dafoe (Google DeepMind)</a>
  <li><a href="https://baobaofzhang.github.io/">Baobao Zhang (FHI, MIT)</a>
    <li><a href="https://jsteinhardt.stat.berkeley.edu/">Jacob Steinhardt (Berkeley and Transluce)</a>
      <li><a href="https://www.robots.ox.ac.uk/~sschulze/">Sebastian Schulze (Oxford)</a>
        <li><a href="https://www.cs.ox.ac.uk/people/yarin.gal/website/">Yarin Gal (Oxford)</a>
      <li><a href="https://mcurmei627.github.io/">Mihaela Curmei (Meta)</a>
      <li><a href="https://andrewilyas.com/">Andrew Ilyas (CMU)</a>
      <li><a href="https://www.jacobh.co.uk/">Jacob Hilton (ARC)</a>
  <li><a href="https://x.com/saprmarks">Sam Marks (Anthropic)</a>
  <li><a href="https://www.cs.toronto.edu/~rgrosse/">Roger Grosse (Anthropic)</a>
  <li><a href="https://www.cs.toronto.edu/~anilcem/">Cem Anil (Anthropic)</a>
  <li><a href="https://www.cognitiverevolution.ai/">Nathan Labenz (Cognitive Revolution)</a>
  
        
</ul>    

          <h2><a name="collaborators">Recommendations</a></h2>
          I recommend Eric Drexler's writing on AI, which I host here to ward against link-rot:
<ul>

  <li><a href="pdfs/Language-for-Intelligent-Machines-A-Prospectus.pdf" target="_blank">Language for Intelligent Machines: A Prospectus (2021) [see paper below for longer treatment]</a></li>
  <li><a href="pdfs/QNRs_FHI-TR-2021-3.0.pdf" target="_blank">QNRs: Toward Language for Intelligent Machines (2021)</a></li>
  <li><a href="pdfs/Reframing_Superintelligence_FHI-TR-2019.pdf" target="_blank">Reframing superintelligence: Comprehensive AI services as general intelligence (2019)</a></li>
      <li><a href="pdfs/MDL-Intelligence-Distillation-for-safe-superintelligent-problem-solving1.pdf" target="_blank">MDL Intelligence Distillation: Exploring strategies for safe access to superintelligent problem-solving capabilities (2015)</a></li>

</ul>

          


        <p class="credits">
          Adapted from Matei Zaharia and <a href="http://andreasviklund.com/">Andreas Viklund</a>.
        </p>
      </div>
    </div>
  

</body></html>
