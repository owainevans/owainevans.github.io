<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Sensory Optimization: Deep Learning and the Cognitive Foundations of Visual Art</title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="/Users/owainevans/notes_owain/aesthetics_notes/patron_optimization/pandoc-templates-master/marked/kultiad-serif.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<header>
<h3 class="date">November 2019</h3>

<h1 class="title">Sensory Optimization: Deep Learning and the Cognitive Foundations of Visual Art</h1>


<h2 class="author">Owain Evans</h2>
<p class="affilation"><em>University of Oxford</em></p>

</header>
<nav id="TOC">
<ul>
<li><a href="#abstract">Abstract</a></li>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#part-1-creating-art-with-networks-for-object-recognition">Part 1: Creating art with networks for object recognition</a><ul>
<li><a href="#can-neural-nets-comprehend-human-paintings">1.1. Can neural nets comprehend human paintings?</a></li>
<li><a href="#generating-images-by-feature-visualization">1.2. Generating images by Feature Visualization</a></li>
<li><a href="#deep-dream-caricatures-and-hybrids">1.2.1. Deep Dream, Caricatures, and Hybrids</a></li>
<li><a href="#deep-dream-art-for-the-neural-net">1.2.2. Deep Dream = art for the neural net</a></li>
<li><a href="#style-transfer-and-medium-transfer">1.3. Style Transfer and Medium Transfer</a></li>
<li><a href="#sensory-optimization-feature-visualization-style-transfer">1.4. Sensory Optimization = Feature Visualization + Style Transfer</a></li>
</ul></li>
<li><a href="#part-2-sensory-optimization-and-human-cognition">Part 2: Sensory Optimization and human cognition</a><ul>
<li><a href="#the-sensory-optimization-hypothesis">2.1. The Sensory Optimization Hypothesis</a></li>
<li><a href="#can-humans-perform-sensory-optimization">2.2. Can humans perform Sensory Optimization?</a></li>
<li><a href="#can-sensory-optimization-create-impressive-art">2.3. Can Sensory Optimization create impressive art?</a></li>
<li><a href="#humans-dont-learn-from-photos">2.4. Humans don’t learn from photos</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#references">References</a></li>
</ul>
</nav>



<!-- TODO:   -->
<!-- - Yoruba (where from? -->
<!-- - credits for ST style images -->
<!-- - credits for other images -->
<!-- Add in new medium transfer -->
<!-- make quote look good -->
<!-- Alt titles -->
<!-- - Sensory Optimization: Deep Learning as a window on the cognitive foundations of Visual Art -->
<!-- - DeepDream is art for neural nets -->
<!-- - Visual art = DeepDream + Style Transfer -->
<!-- - Sensory Optimization: Explaining visual art with deep neural networks -->
<!-- Aeon. Massive Science. -->
<h3 id="abstract">Abstract</h3>
<p>This article is about the cognitive science of visual art. Artists create physical artifacts (such as sculptures or paintings) which depict people, objects, and events. These depictions are usually stylized rather than photo-realistic. How is it that humans are able to understand and create stylized representations? Does this ability depend on general cognitive capacities or an evolutionary adaptation for art? What role is played by learning and culture?</p>
<p>Machine Learning can shed light on these questions. It’s possible to train convolutional neural networks (CNNs) to recognize objects without training them on any visual art. If such CNNs can <em>generalize</em> to visual art (by creating and understanding stylized representations), then CNNs provide a model for how humans could understand art without innate adaptations or cultural learning. I argue that the Deep Dream and Style Transfer algorithms show that CNNs <em>can</em> create a basic form of visual art, and that humans could create art by a similar process. This suggests that artists make art by optimizing for effects on the human object-recognition system. Physical artifacts are optimized to evoke real-world objects for this system (e.g. to evoke people or landscapes) and to serve as superstimuli for this system.</p>
<!-- Machine Learning can shed light on these questions. Convolutional neural networks (CNNs) are trained to recognize objects without getting any exposure to visual art. I make two claims: (1) Deep Dream and Style Transfer show that CNNs can create a basic form of art, and (2) humans could create art by a similar process. Thus CNNs provide a model for understanding stylized representations using only general recognition abilities (i.e. without specialized abilities for art). According to this model, artists create visual art by optimizing for effects on the human object recognition system. Physical artifacts are optimized to evoke real-world objects (e.g. people or animals) for the recognition system and also to serve as superstimuli for features of the recognition system. -->
<!-- I argue that Machine Learning can shed light on these questions. Convolutional neural networks for ImageNet are trained to recognize objects but are not trained on visual art. Nevertheless, these networks can create a basic form of art via the Deep Dream and Style Transfer algorithms. I claim that the human brain could implement a version of these algorithms -->
<!-- This shows that an agent can understand stylized representations through general object recognition abilities, without specialized capacities for art.  -->
<!-- I also argue that the human brain could implement a version of the Deep Dream and Style Transfer algorithms, and so these algorithms provide a model for origin of visual art. According to this model, visual art is produced by optimizing physical artifacts to convey a particular object or scene to the human visual recognition system, and to produce a superstimulus for features used by that system. -->
<!-- Results in deep learning shed light on these questions. We can train convolutional networks to recognize objects without showing them any visual art. If such networks can understand and create visual art, then they provide a computational model for humans that doesn't rely on art-specific abilities. I argue that the techniques of Deep Dream and Style Transfer show that convolutional nets *can* create a basic form of visual art. Moreover, the idea behind these techniques could be implemented by human cognition. On this model, visual art is produced by optimizing physical artifacts to (a) convey a particular object or scene to the human visual system, and to (b) produce a superstimulus for features used by that system. -->
<hr />
<blockquote>
<p>The history of art … may be described as the forging of master keys for opening the mysterious locks of our senses to which only nature herself originally held the key … Like the burglar who tries to break a safe, the artist has no direct access to the inner mechanism. He can only feel his way with sensitive fingers, probing and adjusting his hook or wire when something gives way.</p>
</blockquote>
<p>E.H. Gombrich. <em>Art and Illusion.</em></p>
<h2 id="introduction">Introduction</h2>
<p>Look at the artworks in Figure 1. Each of them is stylized and easily distinguished from a photo. Yet you can effortlessly recognize what is depicted (e.g. people, horses, ducks) and form a judgment about the aesthetic quality of the depictions.</p>
<figure>
<img src="all_figures/1_culture/intro.png" style="width:100.0%" alt="" /><figcaption>Figure 1. Stylized depictions in visual art from different cultures. Left-right: Sumer (3000BC), China (220AD), Japan (1700s), Yoruba (1800s), UK (1935), France (1955).</figcaption>
</figure>
<p>This example raises some foundational questions about visual art and human cognition:</p>
<ul>
<li><p>How do viewers recognize what is depicted in a stylized image and judge the aesthetic quality of the image?</p></li>
<li><p>How do artists create stylized representations that are easily recognizable even to viewers from a distant culture?</p></li>
<li><p>How did humans invent visual art in the first place?</p></li>
</ul>
<p>Answers to these questions will invoke both general human abilities (e.g. vision, manual dexterity) and abilities specialized for visual art. Yet what is the balance between them? Here are two opposing positions:</p>
<blockquote>
<p><em>Art-specific Position</em><br />
On this view, understanding stylized representations requires skills specific to visual art. These skills have two very different sources. Humans could have an evolutionary adaptation for art, an “art instinct” analogous to the language instinct <span class="citation" data-cites="dutton2009art">[<a href="#ref-dutton2009art" role="doc-biblioref">1</a>]</span>. Humans can also <em>learn</em> art-specific skills. These skills could come from immersion in visual art or from being taught an explicit “visual language”, where arbitrary symbols are used to denote concepts <span class="citation" data-cites="goodman1976languages hyman2017depiction">[<a href="#ref-goodman1976languages" role="doc-biblioref">2</a>], [<a href="#ref-hyman2017depiction" role="doc-biblioref">3</a>]</span>.</p>
</blockquote>
<blockquote>
<p><em>Generalist Position</em><br />
On this view, general human abilities are sufficient to understand and create stylized representations. These general abilities depend on innate and learned capacities (e.g. learning to recognize animals) but have nothing to do with art <span class="citation" data-cites="pinker2003how ramachandran1999science">[<a href="#ref-pinker2003how" role="doc-biblioref">4</a>], [<a href="#ref-ramachandran1999science" role="doc-biblioref">5</a>]</span>. This predicts that humans with no exposure to art could understand Figure 1, and that art is accessible across cultures because it exploits general abilities shared by all humans. <!-- This position would suggest that humans invented visual art once their general abilities exceeded a threshold --> In Gombrich’s analogy TODO-LINK, artists are like locksmiths: they forge “master keys for opening the mysterious locks of our senses” <span class="citation" data-cites="gombrich1960art">[<a href="#ref-gombrich1960art" role="doc-biblioref">6</a>]</span>.</p>
</blockquote>
<p>There have been some experimental tests of the Generalist position. In a psychology study in the 1960s, two professors kept their son from seeing any pictures or photos until the age of 19 months. On viewing line-drawings for the first time, the child immediately recognized what was depicted <span class="citation" data-cites="hochberg1962pictorial">[<a href="#ref-hochberg1962pictorial" role="doc-biblioref">7</a>]</span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Yet aside from this study, we have limited data on humans with zero exposure to visual representations. The Generalist position also suggests that there’s no innate art instinct. This is hard to determine today because neuroscience lacks a detailed picture of how the visual system processes art.</p>
<p>Deep neural networks provide a new source of evidence for choosing between the Art-specific and Generalist positions. For the first time in history, there are algorithms for object recognition that approach human performance across a wide range of datasets <span class="citation" data-cites="zoph2018learning litjens2017survey">[<a href="#ref-zoph2018learning" role="doc-biblioref">8</a>], [<a href="#ref-litjens2017survey" role="doc-biblioref">9</a>]</span>. This enables novel computational experiments akin to depriving a child of visual art. It’s possible to train a network to recognize objects (e.g. people, horses, chairs) without giving it any exposure to visual art and then test whether it can understand and create artistic representations. In other words, can a network trained to recognize ordinary objects generalize to visual art? If so, this would support the Generalist position. This “deprivation” experiment for neural nets has not yet been carried out systematically. However similar experiments have: the famous Deep Dream and Style Transfer techniques for generating neural art <span class="citation" data-cites="mordvintsev2015inceptionism gatys2016image">[<a href="#ref-mordvintsev2015inceptionism" role="doc-biblioref">10</a>], [<a href="#ref-gatys2016image" role="doc-biblioref">11</a>]</span>. In this article, I will argue that the results of Deep Dream and Style Transfer show that neural nets for object recognition <em>can</em> generalize to art. This helps to explain how human general abilities could suffice for understanding and creating visual art, and so provides support for the Generalist position.</p>
<p>Here is an outline of the article:</p>
<p>Part 1 reviews evidence that neural nets for object recognition can generate art-like images without having been exposed to visual art during training. I explain Feature Visualization, Deep Dream, and Style Transfer. These techniques create images by optimizing to cause a certain pattern of activation in a trained neural net. The techniques can be combined to produce images that are <em>superstimuli</em> for features of the network (as in Feature Visualization), and <em>transcriptions</em> of content into a different style or medium (as in Style Transfer).</p>
<p>In Part 2, I argue that humans could create art by a similar process to Feature Visualization and Style Transfer. This helps to explain the origins and development of visual art on the Generalist position. Humans do not perform gradient descent but instead apply general intelligence to optimize physical artifacts for the human visual system. <!-- These optimized images can vividly depict objects and individuals and can also be emotionally and aesthetically engaging. --></p>
<!-- [Single figure that summarizes whole article.  -->
<!-- Net trained to recognize objects (recognition network). Images in, internal representation, then labels out.  -->
<!-- Then backprop into image to optimize inputs for this network. End product is art. Then on other side, analogy of humans where we learned to recognize mammoths and art is created to stimulate our neurons.] -->
<h2 id="part-1-creating-art-with-networks-for-object-recognition">Part 1: Creating art with networks for object recognition</h2>
<p>Can neural networks trained to recognize objects generalize to visual art (having been “deprived” of art during training)? Can such networks <em>create</em> visual art? In reviewing the evidence, I limit my discussion to convolutional networks trained to label objects on datasets like ImageNet <span class="citation" data-cites="deng2009imagenet">[<a href="#ref-deng2009imagenet" role="doc-biblioref">12</a>]</span>. Networks trained explicitly to generate images, such as GANs and variational autoencoders, are beyond the scope of this article.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>The ImageNet dataset is not completely free of visual art. A small proportion of images contain art or design.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> Yet it’s not part of the task to recognize what is depicted in this art, and so training on ImageNet is a reasonable approximation of “depriving” a network of art. A related concern is that the network learns something about visual art simply by training on photos, because photos are created and processed for human consumption. I discuss this concern in Section 2.4.</p>
<p>I will introduce some terminology to aid exposition. I refer to a convolutional net trained on ImageNet as a “recognition net”. The experiments described in this article use a few different convolutional architectures (VGG, GoogLeNet and ResNets) and so the term “recognition net” will refer to one of these architectures. <!-- [TODO: possible say here that ideally would all be robust resnet, as this is most human-like, and visualizations most consistently understandable by humans.] --></p>
<h3 id="can-neural-nets-comprehend-human-paintings">1.1. Can neural nets comprehend human paintings?</h3>
<p>The simplest test for whether a recognition net can comprehend visual art is to run the net on paintings and drawings. If there’s a dog in the painting, is it recognized as a dog? Unfortunately, there is not much research on this question. One paper tested YOLO, a conv-net based model, on a range of European paintings <span class="citation" data-cites="redmon2016you">[<a href="#ref-redmon2016you" role="doc-biblioref">14</a>]</span>. Figure 2 shows some of the network’s outputs. The overall results are fairly impressive but substantially below human performance. The results could be improved by applying the latest advances in visual recognition.</p>
<figure>
<img src="all_figures/2_yolo/yolo_compile.png" style="width:100.0%" alt="" /><figcaption>Figure 2. Outputs from applying YOLO to paintings. YOLO is a conv-net based model for recognition and localization. It was not trained on visual art but can generalize to it. In the two leftmost images, YOLO successfully recognizes humans and a dog (reproduced from <span class="citation" data-cites="redmon2016you">[<a href="#ref-redmon2016you" role="doc-biblioref">14</a>]</span>). In the image on the right, YOLOv3 incorrectly identifies people as “stop sign” or “frisbee” (generated by the author using <span class="citation" data-cites="redmon2018yolov3">[<a href="#ref-redmon2018yolov3" role="doc-biblioref">15</a>]</span>).</figcaption>
</figure>
<h3 id="generating-images-by-feature-visualization">1.2. Generating images by Feature Visualization</h3>
<p>One way of using a recognition net to generate art-like images is Feature Visualization (“FV”) and the closely related Deep Dream. Researchers developed FV to help interpret neural networks, by visualizing the features computed by neurons <span class="citation" data-cites="olah2017feature nguyen2016multifaceted">[<a href="#ref-olah2017feature" role="doc-biblioref">16</a>], [<a href="#ref-nguyen2016multifaceted" role="doc-biblioref">17</a>]</span>. To visualize a neuron we synthesize an image that maximizes the neuron’s activation (Fig. 3). The image pixels are iteratively optimized by gradient ascent, backpropagating the activation from the neuron to the pixels. The same technique can be used to visualize a class label (e.g. the “fox” class), a channel, or an entire layer (as in Deep Dream). Intuitively, the idea of FV is to create an image that’s a <em>superstimulus</em> for a neuron without knowing in advance what feature the neuron corresponds to. <!-- This is a *local* optimization process, as it's not possible to jump to the global optimum. --></p>
<figure>
<img src="all_figures/3_feature_viz/diagram_fv.png" style="width:100.0%" alt="" /><figcaption>Figure 3. Schematic diagrams of Feature Visualization and Deep Dream for convolutional nets. Diagram (a) shows FV for class probabilities/labels. At each timestep, the class probability is computed for the image (black arrows) and the image is updated using the gradient of the probability (red arrows). Diagram (b) shows DD, where the objective is the squared activation of one of the conv layers.</figcaption>
</figure>
<p>I will sketch the formal details of FV. Suppose we are generating a visualization for the <span class="math inline">\(i\)</span>-th class label. Let <span class="math inline">\(f_\theta^i(x)\)</span> be the probability that image <span class="math inline">\(x\)</span> is in class <span class="math inline">\(i\)</span>, where <span class="math inline">\(f_\theta\)</span> is the neural net with parameter vector <span class="math inline">\(\theta\)</span> and <span class="math inline">\(x\)</span> is a vector of the image pixels. <!-- ^[To visualize a class label you could also optimize an image for the class logits.] --> The objective is to find the image <span class="math inline">\(x^*\)</span> such that:</p>
<p><span class="math display">\[ x^* = \underset{x}{\mathrm{argmax}}{ ( f_\theta^i(x) )}\]</span></p>
<p>To optimize this objective, we compute the derivative of the probability with respect to the components of <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[\frac{ \partial(f_\theta^i(x)) }{ \partial x }\]</span></p>
<p>This contrasts with the normal use of backpropagation for training the network, where the derivative is taken with respect to components of <span class="math inline">\(\theta\)</span>. To visualize a neuron in one of the middle convolutional layers, we evaluate <span class="math inline">\(f_\theta\)</span> up to that layer and backpropagate from there. For initialization, the image <span class="math inline">\(x\)</span> is set either to random noise or to a photo. Projected Gradient Ascent can be used to keep <span class="math inline">\(x\)</span> from diverging too much from the photo <span class="citation" data-cites="santurkar2019computer">[<a href="#ref-santurkar2019computer" role="doc-biblioref">18</a>]</span>, and image regularization and preconditioning can be used to reduce high-frequency noise <span class="citation" data-cites="olah2017feature">[<a href="#ref-olah2017feature" role="doc-biblioref">16</a>]</span>.</p>
<p>Visualizations for the class labels of a robustly-trained ResNet are shown in Figure 4 <span class="citation" data-cites="santurkar2019computer">[<a href="#ref-santurkar2019computer" role="doc-biblioref">18</a>]</span>. It’s an important and surprising fact that these images resemble animals. The recognition net was trained only to <em>distinguish</em> between animals, not to generate images of them.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> Yet the net implicitly learns a model of how the animals look, which FV is able to extract and visualize.</p>
<figure>
<img src="all_figures/3_feature_viz/fv_madry_compiled.png" style="width:100.0%" alt="" /><figcaption>Figure 4. Visualizations for class labels “dog”, “bird”, and “insect” for a robust ResNet trained on Restricted ImageNet. Images are initialized to a Gaussian fit to training photos and optimized by PGD. Reproduced from <span class="citation" data-cites="santurkar2019computer">[<a href="#ref-santurkar2019computer" role="doc-biblioref">18</a>]</span></figcaption>
</figure>
<p>Visualizations for neurons from the middle layers of a recognition net (GoogleNet) are shown in Figure 5 <span class="citation" data-cites="olah2017feature">[<a href="#ref-olah2017feature" role="doc-biblioref">16</a>]</span>. These neurons code for parts of objects that are useful for classification, such as pointy snouts and the top of a screw. The visualizations look quite different from photos in ImageNet. Like the superstimuli studied in animal behavior research, they are simplified, off-distribution inputs that cause intense activation for the network <span class="citation" data-cites="superstimulus ramachandran1999science">[<a href="#ref-ramachandran1999science" role="doc-biblioref">5</a>], [<a href="#ref-superstimulus" role="doc-biblioref">19</a>]</span>.</p>
<figure>
<img src="all_figures/3_feature_viz/fv_middle_lower_compiled.png" alt="" /><figcaption>Figure 5. Top row: Feature Visualizations for channels in a middle convolutional layer of GoogleNet, which seem to code for pointy dog snouts, the top of screws, and bunches of fruit. Bottom row: Visualizations for channels in an earlier convolutional layer, coding for curved diagonal lines, webbing, spots, grid squares, and pink fabric. Regularization and preconditioning are used in this version of FV. Images reproduced from <span class="citation" data-cites="olah2017feature">[<a href="#ref-olah2017feature" role="doc-biblioref">16</a>]</span>.</figcaption>
</figure>
<p>Neurons in earlier layers seem to compute lower-level features of objects (Fig. 5). Visualizations for these neurons contain abstract, geometric forms, including:</p>
<ul>
<li>blocks of color</li>
<li>straight lines, V-shapes, zigzags, circles.</li>
<li>grids, honeycomb tiling, spots, webs</li>
</ul>
<p>These forms are also found in abstract and decorative art. Indeed, various scientists have hypothesized that people enjoy abstract forms in art because they are superstimuli for geometric features the brain uses to recognize objects.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<h3 id="deep-dream-caricatures-and-hybrids">1.2.1. Deep Dream, Caricatures, and Hybrids</h3>
<p>Deep Dream (“DD”) is a variation on Feature Visualization where an image is optimized for the activation of an entire convolutional layer <span class="citation" data-cites="mordvintsev2015inceptionism">[<a href="#ref-mordvintsev2015inceptionism" role="doc-biblioref">10</a>]</span>.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> Images generated by Deep Dream reflect the distinctions the network cares most about. Training on ImageNet, which requires distinguishing between 120 different dog breeds, produces a preponderance of dogs, whereas the “Places” dataset produces arches and windows.</p>
<p>Figure 6 shows how Deep Dream transforms meatballs into dog heads. This transformation is fairly subtle: if you zoom out the two images look very similar. Yet there’s a big difference in our experience of the images. The transformed image is dense with details like eyes, scales, and insect legs. Deep Dream can do a lot with a little, using subtle changes to evoke an abundance of semantic detail. In this way, Deep Dream is similar to stylized representations in visual art, such as caricatures, which use a small amount of visual information to evoke a particular individual <span class="citation" data-cites="olah2018github">[<a href="#ref-olah2018github" role="doc-biblioref">22</a>]</span>. The artist John Ambrosi has made spectacular use of this “subtle” application of Deep Dream in his series “Dreamscapes.”<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> On the other hand, if Deep Dream is run for longer on an image, the transformation becomes much less subtle (see Fig. 7 left).</p>
<!-- Figure 6 shows how Deep Dream transforms meatballs into dog heads. This transformation is fairly subtle: if you zoom out the two images look very similar. Yet there's a big difference in our experience of the images. The transformed image is dense with details like eyes, scales, and insect legs. Deep Dream does a lot with a little, using subtle changes to evoke an abundance of semantic detail. In this way, Deep Dream is similar to stylized representations in visual art, such as caricatures, which use a small amount of visual information to evoke a particular individual.  -->
<!-- [ Some have claimed that the process by which these efficient representations are discovered is also similar to Deep Dream. Gombrich] -->
<figure>
<img src="all_figures/3_feature_viz/dd_meatballs_compiled.png" style="width:100.0%" alt="" /><figcaption>Figure 6. Deep Dream applied to meatballs.</figcaption>
</figure>
<figure>
<img src="all_figures/3_feature_viz/dd_hybrids_compiled.png" style="width:100.0%" alt="" /><figcaption>Figure 7. Left: Deep Dream applied to a photo of a man. Right: Using FV to add stripes or insect legs to animals. The image is optimized for a particular neuron in the FC layer of a robust ResNet. Images reproduced from <span class="citation" data-cites="engstrom2019learning">[<a href="#ref-engstrom2019learning" role="doc-biblioref">23</a>]</span>.</figcaption>
</figure>
<p>Deep Dream generates bizarre structures that look like hybrid creatures. This kind of hybrid can also be created in a controlled way by FV, by optimizing a photo towards particular features in the recognition net (Fig. 7). It seems that for neural nets, the ability to generate normal animals goes hand-in-hand with the ability to recombine animal parts into hybrids. The same is plausibly true of humans, as hybrid creatures are a common trope in visual art.</p>
<h3 id="deep-dream-art-for-the-neural-net">1.2.2. Deep Dream = art for the neural net</h3>
<p>The images generated by Feature Visualization and Deep Dream have some of the basic properties of human visual art, including accurate representation of natural objects, abstract forms, and caricature. Yet, as I will discuss in Part 2, the images lack global coherence, emotional expressiveness, and variety. My goal in this article is to relate neural networks to questions of how humans understand and create visual art. For this goal, the <em>process</em> of Feature Visualization is as important as the results. The process generates intense or “meaning-dense” stimuli for the recognition net. We could think of this as art <em>for the recognition net</em>, rather than art for humans. It’s noteworthy that humans also find Deep Dream images intense and semantically dense. If the recognition net was trained on a task closer to human learning (e.g. captioning photos of complex social situations), this “art for neural nets” would plausibly get closer to human visual art (see Section 2.3).</p>
<h3 id="style-transfer-and-medium-transfer">1.3. Style Transfer and Medium Transfer</h3>
<p>There are two other properties of human visual art that Feature Visualization does not capture:</p>
<ol type="1">
<li><p><em>Style Re-use</em><br />
Artists create new works that re-use the style of previous works.</p></li>
<li><p><em>Transcription</em><br />
Humans make art in physical media with different formal properties than human visual perception. For example, woodcut prints are 2D, monochrome, and static, whereas human visual perception is a binocular, full-color video stream. (By contrast, FV generates images with the same form as ImageNet photos). So artists <em>transcribe</em> their visual perception into physical media with formal constraints.</p></li>
</ol>
<p>Neural Style Transfer (“ST”) is a technique that uses a recognition net to achieve a simplified version of properties 1 and 2. ST generates an image that fuses the style and content of two source images <span class="citation" data-cites="gatys2016image">[<a href="#ref-gatys2016image" role="doc-biblioref">11</a>]</span>. Like FV, the algorithm for ST optimizes an image to cause a particular response in a recognition net. Yet instead of optimizing for the activation of a neuron, ST optimizes the image to match the internal representation (or “embedding”) of the style and content images under the recognition net (see Figure 8).</p>
<figure>
<img src="all_figures/4_style_transfer/diagram_st_gatys.png" style="width:100.0%" alt="" /><figcaption>Figure 8. Style Transfer. The <em>representation</em> of each image under the recognition net (“conv net”) is computed. The representations are compared using the content and style loss functions and backpropagated to the fusion image. Diagram adapted from <span class="citation" data-cites="mordvintsev2018differentiable gatys2016image">[<a href="#ref-gatys2016image" role="doc-biblioref">11</a>], [<a href="#ref-mordvintsev2018differentiable" role="doc-biblioref">24</a>]</span>.</figcaption>
</figure>
<p>I will sketch the algorithm. As before, the recognition net <span class="math inline">\(f_\theta\)</span> is a convolutional net (such as VGG or robust ResNet) trained on ImageNet. The <em>representation</em> of an image <span class="math inline">\(x\)</span> under <span class="math inline">\(f_\theta\)</span> at the <span class="math inline">\(k\)</span>-th layer is defined as the set of all activations at the layer. The <em>style</em> of image <span class="math inline">\(x\)</span> under <span class="math inline">\(f_\theta\)</span> is defined as a set of spatial statistics (the correlation matrix) of the representations of <span class="math inline">\(x\)</span> at multiple convolutional layers. Thus style depends on low- and mid-level features of <span class="math inline">\(x\)</span> but is invariant to spatial location. The <em>content</em> of <span class="math inline">\(x\)</span> is its representation at one of the later convolutional layers.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> Style Transfer aims to generate a <em>fusion image</em> <span class="math inline">\(x^*\)</span> with style close to the style image <span class="math inline">\(x_s\)</span> and content close to the content image <span class="math inline">\(x_c\)</span>. Thus the objective is a weighted sum of the <em>style loss</em> <span class="math inline">\(L_c\)</span> (the <span class="math inline">\(L_2\)</span> distance between styles) and the content loss <span class="math inline">\(L_c\)</span> (the <span class="math inline">\(L_2\)</span> distance between contents):</p>
<p><span class="math display">\[ x^* = \underset{x}{\mathrm{argmin}}{( \alpha L_c(x,x_c) + \beta L_s(x,x_s))}\]</span></p>
<p>Here <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are the weights for the style and content losses. The definition of style in ST mostly captures small-scale features like colors, textures, and brush strokes, and does not fully capture the richer notion of style found in the study of art history. Nevertheless, the results of ST are surprisingly impressive (Fig. 9) and demonstrate two facts about the recognition net:</p>
<ol type="1">
<li><p>By training on photos, the net has learned features that are general-purpose enough to capture the low-level structure of paintings.</p></li>
<li><p>The net can recognize the content even if the low-level textures differ from anything seen in training: e.g. in Figure 9 the network recognizes a wolf.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p></li>
</ol>
<p><img src="all_figures/4_style_transfer/st_wolf.png" /></p>
<figure>
<img src="all_figures/4_style_transfer/st_dog.png" style="width:100.0%" alt="" /><figcaption>Figure 9. Style Transfer. The style images have contrasting low-level features and correspond to different physical media. This can also be seen as a simplified version of “Medium Transfer”. The fusion images were generated by the author using VGG as in <span class="citation" data-cites="gatys2016image">[<a href="#ref-gatys2016image" role="doc-biblioref">11</a>]</span>, initialized from the content image.</figcaption>
</figure>
<figure>
<img src="all_figures/5_patron_optimization/fv_and_st_table.png" style="width:100.0%" alt="" /><figcaption>Figure 10. Combining Style Transfer and Feature Visualization. The images in the left column are content images generated by FV (first two rows) and DD (third row). Fusion images generated by the author using ST as in Fig. 9.</figcaption>
</figure>
<h4 id="the-physical-medium-of-visual-art">1.3.1. The physical medium of visual art</h4>
<p>The style images for Figure 9 are taken from artworks in different physical media, such as linocut, charcoal drawing and oil painting. Style Transfer is able to transcribe content into fusion images that reflect some of the low-level features of these media. This could be taken a step further by designing physical artifacts rather than digital images. The idea is to replace the soft constraints imposed by the style image by hard constraints imposed by the physical medium (Fig. 11). The physical artifact would be optimized such that, when viewed by the recognition net, it causes a content representation similar to the source content. I refer to this extension of ST as “Medium Transfer”. To implement Medium Transfer, you need to define the space of ways to modify the physical medium. This would be the space of cuts for a woodcut print (Fig. 11) or the space of arrangements of tiles in a mosaic. Some physical artifacts have been designed using ideas similar to Medium Transfer: see Section 1.4 (Perception Engines) and <span class="citation" data-cites="mordvintsev2018differentiable brown2017adversarial">[<a href="#ref-mordvintsev2018differentiable" role="doc-biblioref">24</a>], [<a href="#ref-brown2017adversarial" role="doc-biblioref">26</a>]</span>.</p>
<p>Before computers made digital art possible, human artists had no choice but to work in a physical medium. Yet physical media are also important because they contribute to the viewer’s experience of an artwork.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<figure>
<img src="all_figures/4_style_transfer/diagram_medium_transfer.png" style="width:100.0%" alt="" /><figcaption>Figure 11. Medium Transfer. A possible extension of ST where the style loss is replaced by the hard constraints imposed by a physical medium. In this example, the objective is to make the woodcut print that best matches the content image. The print is created by making cuts into wood and so is constrained to be monochrome and have low spatial resolution.</figcaption>
</figure>
<h3 id="sensory-optimization-feature-visualization-style-transfer">1.4. Sensory Optimization = Feature Visualization + Style Transfer</h3>
<p>Feature Visualization and Style Transfer both optimize an image to cause a particular pattern of activation in a recognition net. The two objective functions can be combined into a single objective and optimized in a unified process. I will introduce the term “Sensory Optimization” (SO) for this kind of process. In particular, a SO process has the following properties:</p>
<ol type="1">
<li><p>Images are created by local optimization to cause a certain pattern of activation in a visual recognition system.</p></li>
<li><p><em>Superstimulus property</em>: images cause higher activation for features of the recognition system than training images.</p></li>
<li><p><em>Transcription property</em>: images are in a different physical medium and style than the training images.</p></li>
</ol>
<p>Combining FV and ST is one example of Sensory Optimization, and results of a simplified version are shown in Figure 10. However SO is not limited to artificial neural nets. Figure 12 illustrates how humans could implement SO. This is similar to ST, but the content images (digital photos) are replaced by human visual input, which is a binocular video stream, and the loss functions now depend on human internal representations of visual inputs. In Part 2, SO in humans is explained in detail.</p>
<figure>
<img src="all_figures/5_patron_optimization/patron_optimization_diagram.png" style="width:100.0%" alt="" /><figcaption>Figure 12: Sensory Optimization for humans. The human visual system (“human vision”) takes the role of conv nets in Figures 3 and 8. The image is optimized to be a superstimulus (as in FV/DD) and also to transcribe the content video into the style of the style image. There are also constraints imposed by the physical medium (as in Fig. 11) but these are not shown. For more explanation of SO in humans, see Section 2.2.</figcaption>
</figure>
<p>Another example of SO was introduced by the artist Tom White in his work <a href="https://medium.com/artists-and-machine-intelligence/perception-engines-8a46bc598d57">“Perception Engines”</a> <span class="citation" data-cites="white2018perception">[<a href="#ref-white2018perception" role="doc-biblioref">27</a>]</span>. White used an algorithm to discover abstract forms that look to a recognition net like everyday objects. This combines FV and ST in one optimization objective. Even more so than Deep Dream, the resulting images “do a lot with a little”, conveying an object class with a low complexity image (Fig. 13). White used the same idea to create abstract superstimuli for a filter for pornographic images <span class="citation" data-cites="white2018synthetic">[<a href="#ref-white2018synthetic" role="doc-biblioref">28</a>]</span>.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> White’s abstract images often resemble (to human eyes) the objects they are intended to depict. The fact that the neural net recognizes this same resemblance is further evidence of generalization from photos to stylized depictions. White also implemented a version of Medium Transfer by making physical prints based on the digital images optimized by SO. The recognition net classified the prints in the same way as the digital images.</p>
<figure>
<img src="all_figures/5_patron_optimization/tom_white_compiled.png" style="width:100.0%" alt="" /><figcaption>Figure 13. Semi-abstract images that are classified as “toilet”, “house tick”, and pornographic (“NSFW”) by recognition nets. From Tom White’s “Perception Engines” and “Pitch Dream” <span class="citation" data-cites="white2018perception">[<a href="#ref-white2018perception" role="doc-biblioref">27</a>]</span>.</figcaption>
</figure>
<h2 id="part-2-sensory-optimization-and-human-cognition">Part 2: Sensory Optimization and human cognition</h2>
<p>In the Introduction, I asked whether general human abilities are sufficient for comprehending and creating visual art (“Generalist position”). Part 1 showed that a neural net trained only on ImageNet can be used to <em>create</em> images that resemble human art. Given how FV and ST work, if a net creates such images it must also <em>comprehend</em> the stylized representations in the images. Part 2 explores the implications of these neural net results for human cognition and the Generalist position.</p>
<h3 id="the-sensory-optimization-hypothesis">2.1. The Sensory Optimization Hypothesis</h3>
<p>I will formulate a hypothesis about how the results from Part 1 relate to visual art in humans. The hypothesis has two premises:</p>
<p><strong>The Sensory Optimization Hypothesis</strong></p>
<ol type="a">
<li><p>If a system <span class="math inline">\(S\)</span> has human-level visual recognition and can perform Sensory Optimization, then <span class="math inline">\(S\)</span> can comprehend and create basic visual art.</p></li>
<li><p>Humans can perform Sensory Optimization.</p></li>
</ol>
<p>I’ll clarify some terms. If system <span class="math inline">\(S\)</span> “performs Sensory Optimization” then <span class="math inline">\(S\)</span> optimizes a physical artifact to cause particular visual effects on <span class="math inline">\(S\)</span>. Neural nets can do this with gradient descent, and in Section 2.2 I claim humans can do this using general intelligence. The term “basic visual art” will be explained in Section 2.3, but roughly means “stylized, expressive representations in a physical medium”.</p>
<p>The Sensory Optimization Hypothesis is a claim that the results from neural nets in Part 1 generalize to humans. This claim has the following implications:</p>
<p>[TODO: possible say here that humans perform SO on themselves or include in section 2.2 or in the previous para!]</p>
<ol type="1">
<li><p>Visual art likely emerged once humans had the general intelligence to perform SO, i.e. to create physical artifacts with the transcription and superstimulus properties. There was no need for a visual language or art-specific evolutionary adaptations.</p></li>
<li><p>The Style Transfer component of SO captures one way artists can borrow from previous work. This borrowing does not <em>require</em> any art-specific abilities. (In practice, artists usually have extensive, art-specific knowledge and experience.)</p></li>
<li><p>We expect children to understand artistic representations from an early age. Adults in cultures without visual art should also be able to understand these representations with minimal instruction. Predictions for non-human primates are less clearcut.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p></li>
</ol>
<!-- [TODO: on SOH, don't need culture for art. It does seem that fairly primitive cultures had amazing art -- Iliad, Babylonian sculpture. Probably true for early Chinese stuff. Mayans. Science actually much more a product of culture because depends on technological base.] -->
<!-- Various psychologists and neuroscientists have believed a version of (1) or (2) that focuses on superstimuli and doesn't mention ST. However, they did not have access to the evidence in Part 1 -->
<p>In the following sections, I will address three objections to the Sensory Optimization Hypothesis and in doing so clarify some of the relevant concepts (“Sensory Optimization”, “basic visual art”, “human-level recognition”). Here are the objections:</p>
<blockquote>
<p><em>2.2. Can humans perform Sensory Optimization?</em><br />
SO can be realized by the FV and ST algorithms applied to conv nets. It’s not clear the human brain can realize this kind of algorithm, as our visual system differs from conv nets and we cannot optimize by gradient descent. Moreover, the creative process in humans does not resemble simple algorithms like FV.</p>
</blockquote>
<blockquote>
<p><em>2.3. Can Sensory Optimization create impressive art?</em><br />
It’s not clear that SO could ever create a set of artworks that matches the range and depth of human art.</p>
</blockquote>
<blockquote>
<p><em>2.4. Humans don’t learn from photos</em><br />
The recognition nets in Part 1 are trained on photos. It’s unclear whether the results about FV and ST would carry over to nets trained on data that is closer to human perceptual input (e.g. something like binocular video).</p>
</blockquote>
<h3 id="can-humans-perform-sensory-optimization">2.2. Can humans perform Sensory Optimization?</h3>
<p>In neural nets, Sensory Optimization can be realized by combining Feature Visualization and Style Transfer (Fig. 12). The human brain cannot implement FV and ST but could implement <em>analogs</em> of these algorithms. This would involve evaluating analogs of the FV and ST objective functions and then optimizing these objectives. I’ll explain the evaluation and optimization processes in turn.</p>
<h4 id="humans-evaluating-the-fv-and-st-objectives">2.2.1. Humans evaluating the FV and ST objectives</h4>
<p>In FV the objective function for an image depends on the activation of neurons in the recognition net. The neurons correspond to classes (e.g. dog), high-level features (e.g. dog ears), and low-level features (e.g. shapes, colors). <!-- If a feature has high activation, this increases the probability of classes which have the feature. --> It’s obvious that humans can recognize classes and features in images. But are we aware (even subconsciously) of the <em>level</em> of activation of features in the brain? This is unclear. However, we are aware of some quantities that seem to be related. People can judge the <em>typicality/prototypicality</em> of objects and features and also judge how <em>diagnostic</em> features are of a class.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> So people could optimize images explicitly for typicality and diagnosticity. Moreover, when people describe an image as vivid, beautiful or interesting, it’s possible they are partly responding to the typicality of elements of the image.<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a></p>
<p>In ST the objective is for the fusion image to match the representations of the style and content images. Humans do not implement this exact algorithm, but they do make similar evaluations. When shown two paintings side-by-side, people without expertise in art can judge how well they match in style.<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> This style comparison is more demanding for humans than neural nets because we have to switch visual attention between the two paintings.<!-- ^[In ST, the neural net directly perceives the fusion, content, and style images. Humans would also see two images side-by-side from slightly different angles and have to adjust for this.]) --></p>
<h4 id="optimization-using-general-intelligence">2.2.2. Optimization using general intelligence</h4>
<p>In neural nets, the optimization for Feature Visualization simultaneously modifies millions of pixels every gradient step. Yet if humans are modifying a painting or drawing, they take actions <em>serially</em>, one stroke at a time, which is a huge speed disadvantage. Humans also evaluate images serially: many eye-movements are required to take in a large, detailed painting <span class="citation" data-cites="chater2018mind">[<a href="#ref-chater2018mind" role="doc-biblioref">13</a>]</span>.</p>
<p>How could humans do Sensory Optimization with this speed disadvantage? It’s important to note that SO in humans is blackbox search. When humans first made art, they had no understanding of the human visual system and no determinstic rules for drawing a picture of an animal that actually looks like the animal. Humans were faced with a trial-and-error search, where exploration is slow due to serial actions. Yet humans had a crucial advantage: general intelligence. People can speed up search by developing an intuition for which actions are promising (as in chess), by hierarchical planning, by analytical techniques (e.g. linear perspective), and by learning from other people. This kind of intelligent local search is also how humans first created practical technologies such as metal tools, architecture, and cooking.</p>
<!-- [We can't rule out humans using gradients subconsciousl or something close to gradients -- that their artistic intuition comes from gradients. Maybe this isn't even art instinct but something that evolved for tool development.] -->
<p>The simplest application of SO in humans is to a single artist painting a particular scene in a particular style. This matches the usual setup for ST in neural nets. Yet this doesn’t account either for the development of style (see Section 2.2.2) or for how the content of paintings depends on previous paintings. In painting a Madonna, medieval artists would make only small changes in style and content to previous Madonnas. This suggests a different “implementation” of SO in humans. As well as being implemented by a single artist, SO can be implemented by a succession of artists in an artistic lineage. Having many artists contribute to the same enterprise allows for more exploration, which is important given the slowness of human search.<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></p>
<h3 id="can-sensory-optimization-create-impressive-art">2.3. Can Sensory Optimization create impressive art?</h3>
<p>Based on Part 1, Sensory Optimization appears to lack the following features of human visual art:</p>
<ol type="1">
<li><p>Coherence: e.g. coherent faces, coherent 3D scenes, symmetrical forms.</p></li>
<li><p>Inventing new styles</p></li>
<li><p>Emotional expression and aesthetic properties (e.g. beauty, elegance)</p></li>
<li><p>Communication of ideas, visual experiences, and narratives</p></li>
</ol>
<p>I believe SO can realize features 1-3, although not at the same level as in human visual art. So the idea of “basic visual art” in the Sensory Optimization Hypothesis is art that has features 1-3, as well as the features of existing FV and ST images. I think SO will be less successful at realizing feature 4.</p>
<h4 id="coherence">2.3.1. Coherence</h4>
<p>In FV and DD images, individual objects are often structurally incoherent (e.g. asymmetric faces or bodies) and the overall arrangement of objects is either stereotyped or random. One possible cause is the local receptive fields of convolutional nets <span class="citation" data-cites="hinton2013taking">[<a href="#ref-hinton2013taking" role="doc-biblioref">36</a>]</span>. Another is that doing well on ImageNet does not require recognizing whether objects or scenes are coherent. I expect general progress in object recognition to mitigate these issues. So if SO is applied to neural nets for vision in the future, I expect more coherent images.</p>
<h4 id="inventing-new-styles">2.3.2. Inventing new styles</h4>
<p>It’s plausible that SO could be extended to develop artistic styles. The idea would be to optimize the “style image” from ST as well as the “fusion image”. As a concrete example, here’s a possible objective function:</p>
<ul>
<li><p><em>FV component</em>: We optimize for class labels related to outdoor scenes and landscapes (e.g. trees, rivers, buildings, clouds, sunlight) and for positive emotions and beauty (see Section 2.3.3 below).</p></li>
<li><p><em>ST component</em>: The content is outdoor scenes and the physical medium is oil paint. We also optimize over the style, by parameterizing the color palette, brush size, thickness of paint, etc.</p></li>
</ul>
<p>We expect optimizing for this objective to yield a style that’s closer to Monet or Turner than to Egon Schiele, but it’s hard to predict. The general idea is that styles are optimized to help create superstimuli (FV) for a certain kind of content. There are many ways to explore this idea experimentally by building on existing work on FV and ST.</p>
<h4 id="emotional-expression-and-aesthetic-properties">2.3.3. Emotional expression and aesthetic properties</h4>
<p>The ability to communicate emotions and feelings is an important property of art. For some theorists it’s <em>the</em> defining property of high art <span class="citation" data-cites="sep-collingwood-aesthetics cooper1992companion">[<a href="#ref-sep-collingwood-aesthetics" role="doc-biblioref">37</a>], [<a href="#ref-cooper1992companion" role="doc-biblioref">38</a>, p. 264]</span>. Images generated by FV and ST are lacking in both the range and clarity of emotional expression. I claim this is because emotion labels were not part of the recognition net’s training. FV can optimize an image to activate “dog” or “dog snout” neurons, but not to activate “sadness”, “fear”, “peacefulness”, or “nostalgia”.</p>
<p>In contrast to ImageNet, human visual experience is associated both with object categories and with emotions. If you see a German Shepherd running at you, you feel fear. If you see someone crying, you infer that the person is sad. A dataset could be created where photos are annotated with this kind of emotional association.<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> Training on such a dataset would allow a neural net to predict both the emotions of people in photos and also the emotions induced by scenes in the photos.<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a></p>
<p>As well as emotions, aesthetic properties play a central role in visual art. Works of art might be beautiful, sublime, elegant, uncanny, garish or (intentionally) dull <span class="citation" data-cites="davies2012artful">[<a href="#ref-davies2012artful" role="doc-biblioref">21</a>, p. 18]</span>. It’s not possible to optimize for these properties having trained on ImageNet. Yet human visual experience comes coupled with aesthetic evaluations. We might see a beautiful tree, an elegant swan, an ugly patch of land. It’s plausible that training on a dataset with aesthetic annotations would help SO generate images with particular aesthetic properties.<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a></p>
<h4 id="communicating-ideas-and-experiences">2.3.4. Communicating ideas and experiences</h4>
<p>Consider a painting that depicts a scene from a particular war. Like a news report, the painting straightforwardly communicates information about the war. However, the painting could also be effective at putting the viewer “in the shoes” of participants in the war, conveying the feelings and thoughts of participants <span class="citation" data-cites="walton1990mimesis">[<a href="#ref-walton1990mimesis" role="doc-biblioref">39</a>]</span>. So the viewer might learn something about this particular war and something about war in general.</p>
<p>It does not seem possible for FV to generate this war painting. FV generates images from scratch that reflect the <em>existing</em> visual knowledge of the recognition net. If the net doesn’t know about the particular war, then FV can’t communicate either basic facts about the war or rich experiences of it. (The war painting could be created by ST with the appropriate content image. But this just passes the buck to selecting the content image.)</p>
<p>Visual art can convey intellectual ideas, as is important in Surrealism, Conceptual Art and many other movements. Visual art can also communicate first-hand experiences of the world, as in the war painting example. It seems difficult for SO to account for these features of art, and so this would be a good topic for further research on SO.</p>
<!-- Visual art can convey ideas (as in Surrealism, Conceptual Art, and Escher) and can communicate first-hand experiences of the world (as in the war painting example). This kind of property of visual art seems difficult for SO to account for and is a good topic for further research.  -->
<h3 id="humans-dont-learn-from-photos">2.4. Humans don’t learn from photos</h3>
<p>The main evidence for the Sensory Optimization Hypothesis comes from running Feature Visualization and Style Transfer on recognition nets trained on ImageNet. The photos in ImageNet do not contain much visual art. Nevertheless, the photos are arguably “one step on the road towards art”; they are static, rectangular, and were processed to make them easy for humans to comprehend. This makes it less surprising that FV/DD images resemble art and that ST works well.</p>
<p>Stronger evidence for the Sensory Optimization Hypothesis could be obtained by training a network on inputs closer to human visual perception. Historically, humans learned to recognize objects by seeing them directly, and not by seeing photos or pictures. We could create a dataset where the inputs are based on a raw binocular video stream, with a shaky camera and varied points of view.</p>
<p>A related concern is that results in Part 1 from convolutional nets would not generalize to other neural architectures (e.g. recurrent models, attention-based models). It would be valuable to see if FV and ST can be “ported” to other architectures, especially those more similar to the human visual system <span class="citation" data-cites="sabour2017dynamic ramachandran2019stand">[<a href="#ref-sabour2017dynamic" role="doc-biblioref">40</a>], [<a href="#ref-ramachandran2019stand" role="doc-biblioref">41</a>]</span>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Neural nets have great potential as a tool for understanding human cognition for visual art. The training data and task objective for a net can be precisely controlled, and we can learn about a trained net’s capabilities by probing its internal representations and testing generalization on novel inputs.</p>
<p>In Part 1, I reviewed evidence about convolutional nets with minimal exposure to visual art. Despite training to recognize objects, these nets can be used to generate images that resemble visual art by the process of Sensory Optimization. SO combines the ideas behind Feature Visualization (superstimuli) and Style Transfer (transcription of content). Images generated by SO can be construed as art “for the neural net” rather than art for human consumption. I suggested ways to extend SO in neural nets to capture more properties of human art (Section 2.3). With richer datasets and tasks, SO could plausibly generate images that have novel artistic styles and that are formally coherent, emotionally expressive, and aesthetically pleasing.</p>
<p>I put forward the Sensory Optimization Hypothesis (Section 2.1). This is claim that SO in neural nets illustrates a general phenomenon: if a system can optimize artifacts to stimulate its own (human-level) visual recognition system, then the system can make basic visual art. Humans could optimize analogs of the FV and ST objectives by applying general intelligence to speed up blackbox search. The Sensory Optimization Hypothesis implies that general human abilities are sufficient for both creating and developing human art, undermining arguments for the necessity of cultural conventions (e.g. a symbolic language) or an innate art instinct.</p>
<p>There are many directions for future research:</p>
<ul>
<li><p>The Sensory Optimization Hypothesis could be tested more systematically, by looking at the images generated by networks with different architectures and more human-like training data (Section 2.4).</p></li>
<li><p>SO could be investigated for visual art forms beyond painting and drawing, such as sculpture, architecture, fashion, cartoons, and movies.</p></li>
<li><p>Section 2.2.2 discussed how SO could help explain the historical development of visual art. To take this idea further, we could train a recognition net on both ImageNet and art up to a particular period and then investigate how this net interprets art from later periods. Building on Section 2.3.2, we could also investigate the kind of styles the net invents.</p></li>
<li><p>The idea underlying SO could be extended to music. Various people have taken a “Generalist position” on the cognitive abilities supporting music and have proposed theories on which music is a superstimulus for human language and auditory perception <span class="citation" data-cites="pinker2003how davies2012artful">[<a href="#ref-pinker2003how" role="doc-biblioref">4</a>], [<a href="#ref-davies2012artful" role="doc-biblioref">21</a>, p. 138]</span>. It’s also plausible that music “transcribes” various human internal experiences or emotional states into a different medium (namely a sequence of sounds). Given recent advances in neural nets for recognizing human speech, it might be possible to generate “music for a neural net”, analogous to the “art for neural nets” generated by FV and ST.</p></li>
</ul>
<hr />
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Thanks to Peli Grietzer, Chris Olah, Justin Manley, Peter Insley, Anders Sandberg, David Krueger, Brian Christian, and Asya Bergal for extremely valuable discussions and feedback about the ideas in this work. I thank Luba Elliott for inviting me to speak at the Creative AI Meetup, which prompted some of the ideas in this paper. I also thank Peli Grietzer for years of fruitful collaboration on related topics.</p>
<h2 id="references" class="unnumbered">References</h2>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-dutton2009art">
<p>[1] D. Dutton, <em>The art instinct: Beauty, pleasure, &amp; human evolution</em>. Oxford University Press, USA, 2009.</p>
</div>
<div id="ref-goodman1976languages">
<p>[2] N. Goodman, <em>Languages of art: An approach to a theory of symbols</em>. Hackett publishing, 1976.</p>
</div>
<div id="ref-hyman2017depiction">
<p>[3] J. Hyman and K. Bantinaki, “Depiction,” in <em>The stanford encyclopedia of philosophy</em>, Summer 2017., E. N. Zalta, Ed. <a href="https://plato.stanford.edu/archives/sum2017/entries/depiction/">https://plato.stanford.edu/archives/sum2017/entries/depiction/</a>; Metaphysics Research Lab, Stanford University, 2017.</p>
</div>
<div id="ref-pinker2003how">
<p>[4] S. Pinker, <em>How the mind works</em>. Penguin UK, 2003.</p>
</div>
<div id="ref-ramachandran1999science">
<p>[5] V. S. Ramachandran and W. Hirstein, “The science of art: A neurological theory of aesthetic experience,” <em>Journal of consciousness Studies</em>, vol. 6, nos. 6-7, pp. 15–51, 1999.</p>
</div>
<div id="ref-gombrich1960art">
<p>[6] E. H. Gombrich, “Art and illusion: A study in the psychology of pictorial representation,” <em>New York: Pantheon</em>, 1960.</p>
</div>
<div id="ref-hochberg1962pictorial">
<p>[7] J. Hochberg and V. Brooks, “Pictorial recognition as an unlearned ability: A study of one child’s performance,” <em>The American Journal of Psychology</em>, vol. 75, no. 4, pp. 624–628, 1962.</p>
</div>
<div id="ref-zoph2018learning">
<p>[8] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning transferable architectures for scalable image recognition,” in <em>Proceedings of the ieee conference on computer vision and pattern recognition</em>, 2018, pp. 8697–8710.</p>
</div>
<div id="ref-litjens2017survey">
<p>[9] G. Litjens <em>et al.</em>, “A survey on deep learning in medical image analysis,” <em>Medical image analysis</em>, vol. 42, pp. 60–88, 2017.</p>
</div>
<div id="ref-mordvintsev2015inceptionism">
<p>[10] A. Mordvintsev, C. Olah, and M. Tyka, “Inceptionism: Going deeper into neural networks, 2015,” <em>Google Research Blog</em>, 2015.</p>
</div>
<div id="ref-gatys2016image">
<p>[11] L. A. Gatys, A. S. Ecker, and M. Bethge, “Image style transfer using convolutional neural networks,” in <em>Proceedings of the ieee conference on computer vision and pattern recognition</em>, 2016, pp. 2414–2423.</p>
</div>
<div id="ref-deng2009imagenet">
<p>[12] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in <em>2009 ieee conference on computer vision and pattern recognition</em>, 2009, pp. 248–255.</p>
</div>
<div id="ref-chater2018mind">
<p>[13] N. Chater, <em>The mind is flat: The illusion of mental depth and the improvised mind</em>. Penguin UK, 2018.</p>
</div>
<div id="ref-redmon2016you">
<p>[14] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once: Unified, real-time object detection,” in <em>Proceedings of the ieee conference on computer vision and pattern recognition</em>, 2016, pp. 779–788.</p>
</div>
<div id="ref-redmon2018yolov3">
<p>[15] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,” <em>arXiv preprint arXiv:1804.02767</em>, 2018.</p>
</div>
<div id="ref-olah2017feature">
<p>[16] C. Olah, A. Mordvintsev, and L. Schubert, “Feature visualization,” <em>Distill</em>, vol. 2, no. 11, p. e7, 2017.</p>
</div>
<div id="ref-nguyen2016multifaceted">
<p>[17] A. Nguyen, J. Yosinski, and J. Clune, “Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks,” <em>arXiv preprint arXiv:1602.03616</em>, 2016.</p>
</div>
<div id="ref-santurkar2019computer">
<p>[18] S. Santurkar, D. Tsipras, B. Tran, A. Ilyas, L. Engstrom, and A. Madry, “Computer vision with a single (robust) classifier,” <em>arXiv preprint arXiv:1906.09453</em>, 2019.</p>
</div>
<div id="ref-superstimulus">
<p>[19] Wikipedia contributors, “Supernormal stimulus — Wikipedia, the free encyclopedia.” <a href="https://en.wikipedia.org/w/index.php?title=Supernormal_stimulus&amp;oldid=920961779&quot;">https://en.wikipedia.org/w/index.php?title=Supernormal_stimulus&amp;oldid=920961779"</a>, 2019.</p>
</div>
<div id="ref-dennett2006breaking">
<p>[20] D. C. Dennett, <em>Breaking the spell: Religion as a natural phenomenon</em>, vol. 14. Penguin, 2006.</p>
</div>
<div id="ref-davies2012artful">
<p>[21] S. Davies, <em>The artful species: Aesthetics, art, and evolution</em>. OUP Oxford, 2012.</p>
</div>
<div id="ref-olah2018github">
<p>[22] Olah, Chris, “Research: Caricatures.” <a href="https://github.com/tensorflow/lucid/issues/121">https://github.com/tensorflow/lucid/issues/121</a>, 2018.</p>
</div>
<div id="ref-engstrom2019learning">
<p>[23] L. Engstrom, A. Ilyas, S. Santurkar, D. Tsipras, B. Tran, and A. Madry, “Learning perceptually-aligned representations via adversarial robustness,” <em>arXiv preprint arXiv:1906.00945</em>, 2019.</p>
</div>
<div id="ref-mordvintsev2018differentiable">
<p>[24] A. Mordvintsev, N. Pezzotti, L. Schubert, and C. Olah, “Differentiable image parameterizations,” <em>Distill</em>, 2018.</p>
</div>
<div id="ref-goodfellow2014explaining">
<p>[25] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial examples,” <em>arXiv preprint arXiv:1412.6572</em>, 2014.</p>
</div>
<div id="ref-brown2017adversarial">
<p>[26] T. B. Brown, D. Mané, A. Roy, M. Abadi, and J. Gilmer, “Adversarial patch,” <em>arXiv preprint arXiv:1712.09665</em>, 2017.</p>
</div>
<div id="ref-white2018perception">
<p>[27] White, Tom, “Perception engines.” <a href="https://medium.com/artists-and-machine-intelligence/perception-engines-8a46bc598d57">https://medium.com/artists-and-machine-intelligence/perception-engines-8a46bc598d57</a>, 2018.</p>
</div>
<div id="ref-white2018synthetic">
<p>[28] White, Tom, “Synthetic abstractions.” <a href="https://medium.com/@tom_25234/synthetic-abstractions-8f0e8f69f390">https://medium.com/@tom_25234/synthetic-abstractions-8f0e8f69f390</a>, 2018.</p>
</div>
<div id="ref-goh">
<p>[29] Goh, Gabriel, “Image synthesis from yahoo’s open nsfw.” <a href="https://open_nsfw.gitlab.io/">https://open_nsfw.gitlab.io/</a>, 2016.</p>
</div>
<div id="ref-deloache1979picture">
<p>[30] J. S. DeLoache, M. S. Strauss, and J. Maynard, “Picture perception in infancy,” <em>Infant Behavior and Development</em>, vol. 2, pp. 77–89, 1979.</p>
</div>
<div id="ref-bloom2002children">
<p>[31] P. Bloom, <em>How children learn the meanings of words</em>. MIT press, 2002.</p>
</div>
<div id="ref-bovet2000picture">
<p>[32] D. Bovet and J. Vauclair, “Picture recognition in animals and humans,” <em>Behavioural brain research</em>, vol. 109, no. 2, pp. 143–165, 2000.</p>
</div>
<div id="ref-murphy2004big">
<p>[33] G. Murphy, <em>The big book of concepts</em>. MIT press, 2004.</p>
</div>
<div id="ref-prototype">
<p>[34] Wikipedia contributors, “Prototype theory — Wikipedia, the free encyclopedia.” <a href="https://en.wikipedia.org/w/index.php?title=Prototype_theory&amp;oldid=907628680&quot;">https://en.wikipedia.org/w/index.php?title=Prototype_theory&amp;oldid=907628680"</a>, 2019.</p>
</div>
<div id="ref-reber2004processing">
<p>[35] R. Reber, N. Schwarz, and P. Winkielman, “Processing fluency and aesthetic pleasure: Is beauty in the perceiver’s processing experience?” <em>Personality and social psychology review: an official journal of the Society for Personality and Social Psychology, Inc</em>, vol. 8, no. 4, pp. 364–382, 2004.</p>
</div>
<div id="ref-hinton2013taking">
<p>[36] G. Hinton, “Taking inverse graphics seriously.” <a href="http://www.csri.utoronto.ca/~hinton/csc2535/notes/lec6b.pdf">http://www.csri.utoronto.ca/~hinton/csc2535/notes/lec6b.pdf</a>.</p>
</div>
<div id="ref-sep-collingwood-aesthetics">
<p>[37] G. Kemp, “Collingwood’s aesthetics,” in <em>The stanford encyclopedia of philosophy</em>, Fall 2016., E. N. Zalta, Ed. <a href="https://plato.stanford.edu/archives/fall2016/entries/collingwood-aesthetics/">https://plato.stanford.edu/archives/fall2016/entries/collingwood-aesthetics/</a>; Metaphysics Research Lab, Stanford University, 2016.</p>
</div>
<div id="ref-cooper1992companion">
<p>[38] D. E. Cooper, J. Margolis, and C. Sartwell, <em>A companion to aesthetics</em>. Blackwell Oxford, UK, 1992.</p>
</div>
<div id="ref-walton1990mimesis">
<p>[39] K. L. Walton, <em>Mimesis as make-believe: On the foundations of the representational arts</em>. Harvard University Press, 1990.</p>
</div>
<div id="ref-sabour2017dynamic">
<p>[40] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between capsules,” in <em>Advances in neural information processing systems</em>, 2017, pp. 3856–3866.</p>
</div>
<div id="ref-ramachandran2019stand">
<p>[41] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, and J. Shlens, “Stand-alone self-attention in vision models,” <em>arXiv preprint arXiv:1906.05909</em>, 2019.</p>
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>This experiment was imperfect because the child had some brief, unintended exposure to visual representations.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>GANs generate impressive artistic images but they seem less informative than ImageNet models about the Generalist vs. Art-specific debate. First, some GANs are trained on human visual art, while ImageNet models are not. Second, GANs are explicitly trained to <em>generate</em> images. We know the human visual system is optimized by evolution for object recognition but we are uncertain about whether it’s also optimized for generating images <span class="citation" data-cites="chater2018mind">[<a href="#ref-chater2018mind" role="doc-biblioref">13</a>]</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>For example, the ImageNet classes for jigsaw puzzles and books<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>In principle it’s possible to distinguish objects without being able to generate pictures of them: e.g. zebras can be distinguished from horses by looking for stripes.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>This hypothesis is discussed by <span class="citation" data-cites="pinker2003how dennett2006breaking ramachandran1999science">[<a href="#ref-pinker2003how" role="doc-biblioref">4</a>], [<a href="#ref-ramachandran1999science" role="doc-biblioref">5</a>], [<a href="#ref-dennett2006breaking" role="doc-biblioref">20</a>]</span>. It’s worth noting that neuroscience does not have a full picture of how low-level features are activated by visual art or why such activation would cause pleasure <span class="citation" data-cites="davies2012artful">[<a href="#ref-davies2012artful" role="doc-biblioref">21</a>]</span>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>More precisely, the objective is to maximize the squared activation of all neurons in a layer.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>See this cathedral <a href="https://www.danielambrosi.com/Dreamscapes-2/i-ZqMVb3M/A">image</a> and this nebula <a href="https://www.danielambrosi.com/Dreamscapes-2/i-kSK9kHH/A">image</a>, as well as the entire <a href="https://www.danielambrosi.com/Dreamscapes-Portal">collection</a>.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p>For full details, including the choice of which convolutional layers to get representations from, see <span class="citation" data-cites="gatys2016image">[<a href="#ref-gatys2016image" role="doc-biblioref">11</a>]</span><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>This might seem trivial, as the fusion image is optimized precisely to be recognizable to the net. However, the fusion images do not appear to be “adversarial” <span class="citation" data-cites="goodfellow2014explaining">[<a href="#ref-goodfellow2014explaining" role="doc-biblioref">25</a>]</span> as humans can also recognize them as satisfying the ST objective.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>The physical medium and the “message” or significance of the artwork can interact. Here are some examples: (1) The use of real gold in religious art. (2) A marble sculpture (e.g. Bernini’s “Rape of Proserpina”) can be especially effective by accurately rendering skin or hair using a hard material. (3) In Degas’s drawings of a woman drying herself with towel, the “trace of the pastel rubbing against the paper’s surface subliminally registers the motion of the towel against the woman’s skin” <span class="citation" data-cites="hyman2017depiction">[<a href="#ref-hyman2017depiction" role="doc-biblioref">3</a>]</span>.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p>Strange quasi-pornographic <a href="https://open_nsfw.gitlab.io/">images</a> have also been generated using Feature Visualization <span class="citation" data-cites="goh">[<a href="#ref-goh" role="doc-biblioref">29</a>]</span>.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12" role="doc-endnote"><p>As well as <span class="citation" data-cites="hochberg1962pictorial">[<a href="#ref-hochberg1962pictorial" role="doc-biblioref">7</a>]</span>, which was discussed in the Introduction, there is evidence that children can recognize objects in pictures before six months <span class="citation" data-cites="deloache1979picture bloom2002children">[<a href="#ref-deloache1979picture" role="doc-biblioref">30</a>], [<a href="#ref-bloom2002children" role="doc-biblioref">31</a>]</span> suggesting this ability does not depend on verbal instruction. The literature on adults from cultures without visual art is more equivocal <span class="citation" data-cites="bovet2000picture">[<a href="#ref-bovet2000picture" role="doc-biblioref">32</a>]</span>, but I’m unaware of any systematic evidence against the SO Hypothesis. There is also evidence showing that primates can spontaneously recognize objects in black-and-white photographs <span class="citation" data-cites="bovet2000picture">[<a href="#ref-bovet2000picture" role="doc-biblioref">32</a>]</span>. Primates’ ability to create visual art seems to be limited, but may be because they lack the general intelligence to perform SO.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13" role="doc-endnote"><p>People can look at a photo and judge whether someone is typical of a boxer or ballerina, and whether someone has typical ears or front teeth. Likewise, people are aware that a tiger’s stripes are diagnostic of a tiger; the stripes distinguish the tiger from other animals. Also see <span class="citation" data-cites="murphy2004big prototype">[<a href="#ref-murphy2004big" role="doc-biblioref">33</a>], [<a href="#ref-prototype" role="doc-biblioref">34</a>]</span>.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14" role="doc-endnote"><p>It has been argued that beauty is associated with typicality and the vividness of caricature with diagnosticity <span class="citation" data-cites="reber2004processing gombrich1960art">[<a href="#ref-gombrich1960art" role="doc-biblioref">6</a>], [<a href="#ref-reber2004processing" role="doc-biblioref">35</a>]</span>.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15" role="doc-endnote"><p>Comparing paintings side-by-side is easier than identifying the style of a single painting.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16" role="doc-endnote"><p>Future research could explore versions of SO in neural nets with multiple distinct recognition nets. This is also relevant to explaining variation in tastes for art among humans. In neural nets, a single recognition net is used for SO but the ultimate consumers of the image are humans. If SO is implemented by multiple artists, there are multiple recognition nets, and the artists must have similar visual systems for this to work. At the same time, variation in visual systems among humans could account for some of the variation in tastes for visual art.<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17" role="doc-endnote"><p>This would be related to existing datasets for image captioning, recognizing human actions, and recognizing emotions from facial expressions.<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18" role="doc-endnote"><p>It’s not clear how well you can predict human emotional responses to abstract paintings from responses to photos of natural scenes. But this is a good topic to investigate.<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19" role="doc-endnote"><p>It’s also plausible that fixing the problem of coherence from Section 2.3.1 would help SO generate images with particular aesthetic properties.<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
